<html>

  <p>This is a demo for <a href="https://htmlday.com/2025">html day 2025</a> which attempts to read html from a device camera and render it on the web.</p>

  <p>Code is at <a href="https://github.com/here/html-day-render-handwriting">github/here/html-day-render-handwriting</a></p>
  
  <!-- Camera video element -->
  <div id="camera-container">
    <button id="start-camera">Start Camera</button>
    <button id="stop-camera">Stop Camera</button>
    <button id="ocr-camera">Camera new OCR Frame</button>
    <br />
    <video id="camera-video" autoplay playsinline muted height="25%"></video>
  </div>
  
  <!-- OCR Engine Selection -->
  <div id="ocr-engine-selection">
    <h3>OCR Engine:</h3>
    <label>
      <input type="radio" name="ocr-engine" value="tesseract" checked> Tesseract.js (Local)
    </label>
    <label>
      <input type="radio" name="ocr-engine" value="openai"> OpenAI GPT-4 Vision
    </label>
    <label>
      <input type="radio" name="ocr-engine" value="claude"> Claude 3 Opus (Anthropic API)
    </label>
    <div id="openai-config" style="display: none; margin-top: 10px;">
      <label for="openai-api-key">OpenAI API Key:</label>
      <input type="password" id="openai-api-key" placeholder="Enter your OpenAI API key">
      <br>
      <label for="openai-endpoint">Endpoint URL (optional):</label>
      <input type="text" id="openai-endpoint" placeholder="https://api.openai.com (default)">
      <br>
      <small style="color: #666;">
        <strong>Note:</strong> OpenAI API requires credits. Direct API calls may fail due to CORS.
        <br><strong>For best results:</strong> Run <code>npm install && npm start</code> for proxy server
      </small>
    </div>
    <div id="claude-config" style="display: none; margin-top: 10px;">
      <label for="claude-api-key">Anthropic API Key:</label>
      <input type="password" id="claude-api-key" placeholder="Enter your Anthropic API key">
      <br>
      <label for="claude-endpoint">Endpoint URL (optional):</label>
      <input type="text" id="claude-endpoint" placeholder="https://api.anthropic.com (default)">
      <br>
      <small style="color: #666;">
        <strong>Note:</strong> The app now uses a Digital Ocean Functions proxy to handle CORS issues.
        <br><strong>Recommended:</strong>
        <br>‚úÖ <strong>Tesseract.js</strong> - Works offline, no credits needed, no setup required
        <br>‚ö†Ô∏è <strong>Claude API</strong> - Requires Anthropic credits, uses cloud proxy
        <br><strong>üí° Tip:</strong> If you're getting credit balance errors, Tesseract.js is the best free alternative!
      </small>
    </div>
  </div>
  
    <div id="log"></div>
  
    <div id="captured-frame-container">
      <h3>Last Captured Frame:</h3>
      <canvas id="captured-frame" style="border: 2px solid #ccc; max-width: 100%; height: 25%;"></canvas>
    </div>
  
  <img height="300px" id="htmlhandwritten" src="knocknoc2.jpg">
  <p>image credit <a href="https://www.johncarr.online/knock-knock-jokes/">johncarr.online/knock-knock-jokes</a></p>

  <p>Start local server with `python3 -m http.server` and view at http://127.0.0.1:8000/</p>
  <p>It uses <a href="https://github.com/naptha/tesseract.js">tesseract.js</a> to perform OCR on the camera frame.</p>
  <p>It uses <a href="https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/getUserMedia">getUserMedia</a> to access the camera.</p>
  <p>It uses <a href="https://developer.mozilla.org/en-US/docs/Web/API/Canvas_API">Canvas API</a> to draw the camera frame and the OCR result.</p>
  <p>It uses <a href="https://developer.mozilla.org/en-US/docs/Web/API/WebRTC_API">WebRTC API</a> to access the camera.</p>
  <p>It uses <a href="https://developer.mozilla.org/en-US/docs/Web/API/WebRTC_API">WebRTC API</a> to access the camera.</p>

  <!--  <img src="https://herebox.org/wp-content/uploads/2025/07/the-triplets-of-belleville-749x1024.jpg"> -->
 
    <script type="module">
        import tesseractJs from "https://esm.sh/tesseract.js@6.0.1";

        // Get the image and log elements from the DOM
        const imageElement = document.getElementById('htmlhandwritten');
        const logElement = document.getElementById('log');
        
        // OCR engine selection elements
        const ocrEngineRadios = document.querySelectorAll('input[name="ocr-engine"]');
        const openaiConfig = document.getElementById('openai-config');
        const openaiApiKey = document.getElementById('openai-api-key');
        const openaiEndpoint = document.getElementById('openai-endpoint');
        const claudeConfig = document.getElementById('claude-config');
        const claudeApiKey = document.getElementById('claude-api-key');
        const claudeEndpoint = document.getElementById('claude-endpoint');
        
        // Function to get selected OCR engine
        function getSelectedOCREngine() {
          return document.querySelector('input[name="ocr-engine"]:checked').value;
        }
        
        // Function to update UI based on OCR engine selection
        function updateOCREngineUI() {
          const selectedEngine = getSelectedOCREngine();
          
          // Hide all config sections
          openaiConfig.style.display = 'none';
          claudeConfig.style.display = 'none';
          
          // Show the appropriate config section
          if (selectedEngine === 'openai') {
            openaiConfig.style.display = 'block';
          } else if (selectedEngine === 'claude') {
            claudeConfig.style.display = 'block';
          }
        }
        
        // Event listeners for OCR engine selection
        ocrEngineRadios.forEach(radio => {
          radio.addEventListener('change', updateOCREngineUI);
        });
        
        // Function to update the log/status on the page
        function updateLog(status) {
          logElement.innerHTML = `<p>${status}</p>`;
          console.log(status);
        }
        
        // Function to convert canvas to base64
        function canvasToBase64(canvas) {
          return canvas.toDataURL('image/jpeg', 0.8);
        }
        
        // Claude 3 Opus OCR function
        async function performClaudeOCR(imageData) {
          const apiKey = claudeApiKey.value.trim();
          let endpoint = claudeEndpoint.value.trim() || 'https://api.anthropic.com';
          
          if (!apiKey) {
            throw new Error('Please provide your Anthropic API key for Claude OCR');
          }
          
          updateLog('Performing OCR with Claude 3 Opus...');
          
          const prompt = `Please perform OCR (Optical Character Recognition) on this image and extract all the text content. Return only the extracted text without any additional formatting or explanations.`;
          
          const requestBody = {
            model: "claude-3-5-sonnet-20241022",
            max_tokens: 1000,
            messages: [
              {
                role: "user",
                content: [
                  {
                    type: "text",
                    text: prompt
                  },
                  {
                    type: "image",
                    source: {
                      type: "base64",
                      media_type: "image/jpeg",
                      data: imageData.split(',')[1] // Remove data:image/jpeg;base64, prefix
                    }
                  }
                ]
              }
            ]
          };
          
          try {
            // Try multiple approaches to handle CORS
            const approaches = [
              // Approach 1: Direct API call (may fail due to CORS)
              async () => {
                updateLog('Trying direct API call...');
                const response = await fetch(`${endpoint}/v1/messages`, {
                  method: 'POST',
                  headers: {
                    'x-api-key': apiKey,
                    'anthropic-version': '2023-06-01',
                    'Content-Type': 'application/json'
                  },
                  body: JSON.stringify(requestBody)
                });
                return response;
              },
              // Approach 2: Digital Ocean Functions proxy
              async () => {
                updateLog('Trying Digital Ocean Functions proxy...');
                const response = await fetch('https://faas-sfo3-7872a1dd.doserverless.co/api/v1/web/fn-00585b33-7c19-4732-b11d-1ca044549063/default/ocr-proxy', {
                  method: 'POST',
                  headers: {
                    'Content-Type': 'application/json'
                  },
                  body: JSON.stringify({
                    method: 'POST',
                    path: 'messages',
                    headers: {
                      'x-api-key': apiKey,
                      'anthropic-version': '2023-06-01'
                    },
                    body: JSON.stringify(requestBody),
                    api_type: 'anthropic'
                  })
                });
                return response;
              },
              // Approach 3: CORS proxy (fallback)
              async () => {
                updateLog('Trying CORS proxy...');
                const response = await fetch('https://cors-anywhere.herokuapp.com/https://api.anthropic.com/v1/messages', {
                  method: 'POST',
                  headers: {
                    'x-api-key': apiKey,
                    'anthropic-version': '2023-06-01',
                    'Content-Type': 'application/json',
                    'Origin': window.location.origin
                  },
                  body: JSON.stringify(requestBody)
                });
                return response;
              }
            ];
            
            let lastError;
            
            for (let i = 0; i < approaches.length; i++) {
              try {
                const response = await approaches[i]();
                
                if (!response.ok) {
                  const errorText = await response.text();
                  lastError = `Approach ${i + 1} failed: ${response.status} - ${errorText}`;
                  console.log(lastError);
                  continue;
                }
                
                const result = await response.json();
                
                if (result.content && result.content[0] && result.content[0].text) {
                  return result.content[0].text.trim();
                } else {
                  throw new Error('Unexpected response format from Claude API');
                }
                
              } catch (error) {
                lastError = `Approach ${i + 1} error: ${error.message}`;
                console.log(lastError);
                continue;
              }
            }
            
            // If all approaches failed, provide helpful guidance
            if (lastError && lastError.includes('credit balance is too low')) {
              throw new Error(`Anthropic API requires credits. Your account has insufficient credits.\n\nSolutions:\n1. Add credits to your Anthropic account\n2. Use Tesseract.js instead (works offline, no credits needed)\n3. Try a different OCR service\n\nError: ${lastError}`);
            } else {
              throw new Error(`All API approaches failed. Consider:\n1. Check your Anthropic API key is valid\n2. Ensure the proxy server is running (npm start)\n3. Use Tesseract.js instead (works offline)\n\nLast error: ${lastError}`);
            }
            
          } catch (error) {
            console.error('Claude API error:', error);
            throw error;
          }
        }
        
        // OpenAI GPT-4 Vision OCR function
        async function performOpenAIOCR(imageData) {
          const apiKey = openaiApiKey.value.trim();
          let endpoint = openaiEndpoint.value.trim() || 'https://api.openai.com';
          
          if (!apiKey) {
            throw new Error('Please provide your OpenAI API key for GPT-4 Vision OCR');
          }
          
          updateLog('Performing OCR with OpenAI GPT-4 Vision...');
          
          const prompt = `Please perform OCR (Optical Character Recognition) on this image and extract all the text content. Return only the extracted text without any additional formatting or explanations.`;
          
          const requestBody = {
            model: "gpt-4o",
            messages: [
              {
                role: "user",
                content: [
                  {
                    type: "text",
                    text: prompt
                  },
                  {
                    type: "image_url",
                    image_url: {
                      url: imageData
                    }
                  }
                ]
              }
            ],
            max_tokens: 1000,
            temperature: 0.1
          };
          
          try {
            const response = await fetch(`${endpoint}/v1/chat/completions`, {
              method: 'POST',
              headers: {
                'Authorization': `Bearer ${apiKey}`,
                'Content-Type': 'application/json'
              },
              body: JSON.stringify(requestBody)
            });
            
            if (!response.ok) {
              const errorText = await response.text();
              throw new Error(`OpenAI API error: ${response.status} - ${errorText}`);
            }
            
            const result = await response.json();
            
            if (result.choices && result.choices[0] && result.choices[0].message) {
              return result.choices[0].message.content.trim();
            } else {
              throw new Error('Unexpected response format from OpenAI API');
            }
            
          } catch (error) {
            console.error('OpenAI API error:', error);
            throw error;
          }
        }
        
        // The main OCR function
        async function performOCR() {
          const selectedEngine = getSelectedOCREngine();
          
          // Check credentials for AI engines
          if (selectedEngine === 'openai') {
            const apiKey = openaiApiKey.value.trim();
            if (!apiKey) {
              updateLog('OpenAI selected but API key not configured. Please enter your OpenAI API key, or switch to Tesseract.');
              return;
            }
          } else if (selectedEngine === 'claude') {
            const apiKey = claudeApiKey.value.trim();
            if (!apiKey) {
              updateLog('Claude selected but API key not configured. Please enter your Anthropic API key, or switch to Tesseract.');
              return;
            }
          }
          
          updateLog(`Recognizing text using ${selectedEngine}... This may take a moment.`);
        
          try {
            let text;
            
            if (selectedEngine === 'openai') {
              // For OpenAI, we need to convert the image to base64
              const canvas = document.createElement('canvas');
              const ctx = canvas.getContext('2d');
              canvas.width = imageElement.naturalWidth;
              canvas.height = imageElement.naturalHeight;
              ctx.drawImage(imageElement, 0, 0);
              
              const imageData = canvasToBase64(canvas);
              text = await performOpenAIOCR(imageData);
              
              // Display the results
              updateLog('OpenAI OCR Complete!');
              logElement.innerHTML += `<h2>Extracted Text:</h2><pre>${text}</pre>`;
              
            } else if (selectedEngine === 'claude') {
              // For Claude, we need to convert the image to base64
              const canvas = document.createElement('canvas');
              const ctx = canvas.getContext('2d');
              canvas.width = imageElement.naturalWidth;
              canvas.height = imageElement.naturalHeight;
              ctx.drawImage(imageElement, 0, 0);
              
              const imageData = canvasToBase64(canvas);
              text = await performClaudeOCR(imageData);
              
              // Display the results
              updateLog('Claude OCR Complete!');
              logElement.innerHTML += `<h2>Extracted Text:</h2><pre>${text}</pre>`;
              
            } else {
              // Use Tesseract.js
              const result = await tesseractJs.recognize(
                imageElement,
                'eng', // Language code (e.g., 'eng' for English)
                {
                  logger: m => {
                    // Log progress to the console and update the UI
                    const progressStatus = `${m.status} (${(m.progress * 100).toFixed(2)}%)`;
                    updateLog(progressStatus);
                  }
                }
              );
              
              // Debug: Log the full result structure
              console.log('Full OCR result:', result);
              
              const { data: { text: extractedText, words } } = result;
              text = extractedText;
          
              // Display the full extracted text
              updateLog('Tesseract OCR Complete!');
              logElement.innerHTML += `<h2>Full Text:</h2><pre>${text}</pre>`;
          
              // Display individual words and their confidence levels (if available)
              if (words && Array.isArray(words)) {
                logElement.innerHTML += '<h2>Individual Words:</h2>';
                const wordList = words.map(w => `<li>${w.text} (Confidence: ${w.confidence.toFixed(2)}%)</li>`).join('');
                logElement.innerHTML += `<ul>${wordList}</ul>`;
              } else {
                logElement.innerHTML += '<p><em>Word-level data not available</em></p>';
              }
            }
        
          } catch (error) {
            console.error('OCR failed:', error);
            updateLog(`Failed to recognize text: ${error.message}`);
          }
        }
        
        // Run the OCR function when the image has loaded
        imageElement.onload = () => {
          performOCR();
        };
        
        // If the image is already cached/loaded, run it immediately
        if (imageElement.complete) {
          performOCR();
        }
        
        // Camera functionality
        const videoElement = document.getElementById('camera-video');
        const startButton = document.getElementById('start-camera');
        const stopButton = document.getElementById('stop-camera');
        const ocrButton = document.getElementById('ocr-camera');
        let stream = null;
        
        // Start camera function
        async function startCamera() {
          try {
            updateLog('Requesting camera permissions...');
            
            // Request camera access with specific constraints for mobile
            stream = await navigator.mediaDevices.getUserMedia({
              video: {
                facingMode: 'environment', // Use back camera on mobile
                width: { ideal: 1280 },
                height: { ideal: 720 }
              },
              audio: false
            });
            
            // Set the video source
            videoElement.srcObject = stream;
            
            updateLog('Camera started successfully!');
            startButton.style.display = 'none';
            stopButton.style.display = 'inline-block';
            
          } catch (error) {
            console.error('Camera access failed:', error);
            updateLog(`Camera access failed: ${error.message}`);
          }
        }
        
        // Stop camera function
        function stopCamera() {
          if (stream) {
            stream.getTracks().forEach(track => track.stop());
            stream = null;
            videoElement.srcObject = null;
            updateLog('Camera stopped');
            startButton.style.display = 'inline-block';
            stopButton.style.display = 'none';
          }
        }
        
        // OCR function for camera video frame
        async function performOCROnCamera() {
          if (!stream || !videoElement.srcObject) {
            updateLog('Please start the camera first');
            return;
          }
          
          const selectedEngine = getSelectedOCREngine();
          
          // Check credentials for AI engines
          if (selectedEngine === 'openai') {
            const apiKey = openaiApiKey.value.trim();
            if (!apiKey) {
              updateLog('OpenAI selected but API key not configured. Please enter your OpenAI API key, or switch to Tesseract.');
              return;
            }
          } else if (selectedEngine === 'claude') {
            const apiKey = claudeApiKey.value.trim();
            if (!apiKey) {
              updateLog('Claude selected but API key not configured. Please enter your Anthropic API key, or switch to Tesseract.');
              return;
            }
          }
          
          updateLog(`Performing OCR on camera frame using ${selectedEngine}...`);
          
          try {
            // Get the display canvas for showing the captured frame
            const displayCanvas = document.getElementById('captured-frame');
            const displayCtx = displayCanvas.getContext('2d');
            
            // Set display canvas size to match video dimensions
            displayCanvas.width = videoElement.videoWidth;
            displayCanvas.height = videoElement.videoHeight;
            
            // Draw the current video frame to the display canvas
            displayCtx.drawImage(videoElement, 0, 0, displayCanvas.width, displayCanvas.height);
            
            // Create a temporary canvas for OCR processing
            const canvas = document.createElement('canvas');
            const ctx = canvas.getContext('2d');
            
            // Set canvas size to match video dimensions
            canvas.width = videoElement.videoWidth;
            canvas.height = videoElement.videoHeight;
            
            // Draw the current video frame to the canvas
            ctx.drawImage(videoElement, 0, 0, canvas.width, canvas.height);
            
            if (selectedEngine === 'openai') {
              // Use OpenAI for camera OCR
              const imageData = canvasToBase64(canvas);
              const text = await performOpenAIOCR(imageData);
              
              // Display the results
              updateLog('Camera OpenAI OCR Complete!');
              logElement.innerHTML += `<h2>Camera Frame Text:</h2><pre>${text}</pre>`;
              
            } else if (selectedEngine === 'claude') {
              // Use Claude for camera OCR
              const imageData = canvasToBase64(canvas);
              const text = await performClaudeOCR(imageData);
              
              // Display the results
              updateLog('Camera Claude OCR Complete!');
              logElement.innerHTML += `<h2>Camera Frame Text:</h2><pre>${text}</pre>`;
              
            } else {
              // Use Tesseract for camera OCR
              const result = await tesseractJs.recognize(
                canvas,
                'eng',
                {
                  logger: m => {
                    const progressStatus = `Camera OCR: ${m.status} (${(m.progress * 100).toFixed(2)}%)`;
                    updateLog(progressStatus);
                  }
                }
              );
              
              // Debug: Log the full result structure
              console.log('Camera OCR result:', result);
              
              const { data: { text, words } } = result;
              
              // Display the results
              updateLog('Camera Tesseract OCR Complete!');
              logElement.innerHTML += `<h2>Camera Frame Text:</h2><pre>${text}</pre>`;
              
              // Display individual words and their confidence levels (if available)
              if (words && Array.isArray(words)) {
                logElement.innerHTML += '<h2>Camera Frame Words:</h2>';
                const wordList = words.map(w => `<li>${w.text} (Confidence: ${w.confidence.toFixed(2)}%)</li>`).join('');
                logElement.innerHTML += `<ul>${wordList}</ul>`;
              } else {
                logElement.innerHTML += '<p><em>Word-level data not available</em></p>';
              }
            }
            
          } catch (error) {
            console.error('Camera OCR failed:', error);
            updateLog(`Camera OCR failed: ${error.message}`);
          }
        }
        
        // Event listeners for camera buttons
        startButton.addEventListener('click', startCamera);
        stopButton.addEventListener('click', stopCamera);
        ocrButton.addEventListener('click', performOCROnCamera);
        
        // Test API connection function
        async function testAPIConnection() {
          const apiKey = claudeApiKey.value.trim();
          if (!apiKey) {
            updateLog('Please enter your Anthropic API key first');
            return;
          }
          
          updateLog('Testing API connection...');
          
          try {
            const response = await fetch('https://faas-sfo3-7872a1dd.doserverless.co/api/v1/web/fn-00585b33-7c19-4732-b11d-1ca044549063/default/ocr-proxy', {
              method: 'POST',
              headers: {
                'Content-Type': 'application/json'
              },
              body: JSON.stringify({
                method: 'POST',
                path: 'test-anthropic',
                headers: {
                  'x-api-key': apiKey
                },
                api_type: 'anthropic'
              })
            });
            
            const result = await response.json();
            
            if (result.statusCode === 200) {
              const body = JSON.parse(result.body);
              if (body.status === 400 && body.response && body.response.includes('credit balance is too low')) {
                updateLog('‚ùå API Test Failed: Insufficient credits');
                updateLog('Your Anthropic account needs more credits to use the API.');
                updateLog('');
                updateLog('Troubleshooting:');
                updateLog('1. Check your Anthropic billing dashboard');
                updateLog('2. Credits may take 5-10 minutes to process');
                updateLog('3. Ensure you added credits to the correct account');
                updateLog('4. Try refreshing your API key');
                updateLog('');
                updateLog('Alternative Solutions:');
                updateLog('‚úÖ Use Tesseract.js (works offline, no credits needed)');
                updateLog('üîß Try a different OCR service');
                updateLog('‚è∞ Wait 10 minutes and test again');
              } else if (body.status === 200) {
                updateLog('‚úÖ API Test Successful: Your API key is working!');
              } else {
                updateLog(`‚ùå API Test Failed: ${body.status} - ${body.response || body.error}`);
              }
            } else {
              updateLog(`‚ùå Proxy Test Failed: ${result.statusCode} - ${result.body}`);
            }
          } catch (error) {
            updateLog(`‚ùå API Test Failed: ${error.message}`);
            updateLog('Make sure the Digital Ocean Functions proxy is accessible');
          }
        }
        
        // Add test button to Claude config
        const testButton = document.createElement('button');
        testButton.textContent = 'Test API Connection';
        testButton.onclick = testAPIConnection;
        testButton.style.marginTop = '10px';
        claudeConfig.appendChild(testButton);
        
        // Add switch to Tesseract button
        const switchButton = document.createElement('button');
        switchButton.textContent = 'Switch to Tesseract (Free)';
        switchButton.onclick = () => {
          document.querySelector('input[value="tesseract"]').checked = true;
          updateOCREngineUI();
          updateLog('‚úÖ Switched to Tesseract.js - No credits needed!');
        };
        switchButton.style.marginTop = '5px';
        switchButton.style.marginLeft = '10px';
        switchButton.style.backgroundColor = '#4CAF50';
        switchButton.style.color = 'white';
        switchButton.style.border = 'none';
        switchButton.style.padding = '5px 10px';
        switchButton.style.borderRadius = '3px';
        switchButton.style.cursor = 'pointer';
        claudeConfig.appendChild(switchButton);
        
        // Initially hide the stop button
        stopButton.style.display = 'none';
        
        // Initialize OCR engine UI
        updateOCREngineUI();
    </script>

</html>