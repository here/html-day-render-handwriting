<html>
  <style>
    body {
      text-align: center;
      font-family: Arial, sans-serif;
      max-width: 1200px;
      margin: 0 auto;
      padding: 20px;
    }
    
    .camera-row {
      display: flex;
      justify-content: center;
      align-items: flex-start;
      gap: 20px;
      margin: 20px 0;
    }
    
    .camera-section, .frame-section {
      flex: 1;
      max-width: 400px;
    }
    
    .camera-section h3, .frame-section h3 {
      margin-top: 0;
      margin-bottom: 10px;
    }
    
    #camera-video, #captured-frame {
      width: auto;
      height: 200px;
      border: 2px solid #ccc;
      border-radius: 8px;
    }
    
    button {
      margin: 5px;
      padding: 8px 16px;
      border: none;
      border-radius: 4px;
      background-color: #007bff;
      color: white;
    }
    
    button:hover {
      background-color: #0056b3;
    }
    
    #ocr-engine-selection {
      margin: 20px 0;
      text-align: left;
      max-width: 600px;
      margin-left: auto;
      margin-right: auto;
    }
    
    #log {
      margin: 20px 0;
      text-align: left;
      max-width: 800px;
      margin-left: auto;
      margin-right: auto;
      padding: 15px;
      background-color: #f8f9fa;
      border-radius: 8px;
      border: 1px solid #dee2e6;
    }
    
    input[type="text"], input[type="password"] {
      width: 100%;
      padding: 8px;
      margin: 5px 0;
      border: 1px solid #ccc;
      border-radius: 4px;
    }
    
    label {
      display: block;
      margin: 10px 0 5px 0;
      font-weight: bold;
    }
  </style>

  <h1>OCR Text Recognition Demo</h1>
  <p>This is a demo for <a href="https://htmlday.com/2025">html day 2025</a> which attempts to read html from a device camera and render it on the web.</p>

  <p>Code is at <a href="https://github.com/here/html-day-render-handwriting">github/here/html-day-render-handwriting</a></p>
  
  <!-- Camera and Frame Row -->
  <div class="camera-row">
    <div class="camera-section">
      <h3>Camera View:</h3>
      <video id="camera-video" autoplay playsinline muted></video>
    </div>
    
    <div class="frame-section">
      <h3>Last Captured Frame:</h3>
      <canvas id="captured-frame"></canvas>
    </div>
  </div>

  <div>
    <button id="start-camera">Start Camera</button>
    <button id="stop-camera">Stop Camera</button>
    <button id="ocr-camera">Take new snapshot from camera</button>
    <button id="retry-ocr">Try OCR text recognition with current snapshot</button>
  </div>
  
    <div id="log"></div>


  <!-- OCR Engine Selection -->
  <div id="ocr-engine-selection">
    <h3>OCR Engine:</h3>
    <label>
      <input type="radio" name="ocr-engine" value="tesseract" checked> Tesseract.js (Local)
    </label>
    <label>
      <input type="radio" name="ocr-engine" value="openai"> OpenAI GPT-4 Vision
    </label>
    <label style="display: none;">
      <input type="radio" name="ocr-engine" value="claude"> Claude 3 Opus (Anthropic API)
    </label>
    <div id="openai-config" style="display: none; margin-top: 10px;">
      <label for="openai-api-key">OpenAI API Key (optional):</label>
      <input type="password" id="openai-api-key" placeholder="Enter your OpenAI API key">
      <br>
      <label for="openai-endpoint">Endpoint URL (optional):</label>
      <input type="text" id="openai-endpoint" placeholder="https://api.openai.com (default)">

    </div>
    <div id="claude-config" style="display: none; margin-top: 10px;">
      <label for="claude-api-key">Anthropic API Key:</label>
      <input type="password" id="claude-api-key" placeholder="Enter your Anthropic API key">
      <br>
      <label for="claude-endpoint">Endpoint URL (optional):</label>
      <input type="text" id="claude-endpoint" placeholder="https://api.anthropic.com (default)">
      <br>
      <small style="color: #666;">
        <strong>Note:</strong> The app now uses a Digital Ocean Functions proxy to handle CORS issues.
        <br><strong>Recommended:</strong>
        <br>‚úÖ <strong>Tesseract.js</strong> - Works offline, no credits needed, no setup required
        <br>‚ö†Ô∏è <strong>Claude API</strong> - Requires Anthropic credits, uses cloud proxy
        <br><strong>üí° Tip:</strong> If you're getting credit balance errors, Tesseract.js is the best free alternative!
      </small>
    </div>
  </div>
  
      <div style="margin: 30px 0;">
      <h3>Default Image</h3>
      <img height="300px" id="htmlhandwritten" src="knocknoc2.jpg" style="border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
      <p>image credit <a href="https://www.johncarr.online/knock-knock-jokes/">johncarr.online/knock-knock-jokes</a></p>
    </div>

  <p>Start local server with `python3 -m http.server` and view at http://127.0.0.1:8000/</p>
  <p>It uses <a href="https://github.com/naptha/tesseract.js">tesseract.js</a> to perform OCR on the camera frame.</p>
  <p>It uses <a href="https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/getUserMedia">getUserMedia</a> to access the camera.</p>
  <p>It uses <a href="https://developer.mozilla.org/en-US/docs/Web/API/Canvas_API">Canvas API</a> to draw the camera frame and the OCR result.</p>
  <p>It uses <a href="https://developer.mozilla.org/en-US/docs/Web/API/WebRTC_API">WebRTC API</a> to access the camera.</p>
  <p>It uses <a href="https://developer.mozilla.org/en-US/docs/Web/API/WebRTC_API">WebRTC API</a> to access the camera.</p>

  <!--  <img src="https://herebox.org/wp-content/uploads/2025/07/the-triplets-of-belleville-749x1024.jpg"> -->
 
    <script type="module">
        import tesseractJs from "https://esm.sh/tesseract.js@6.0.1";

        // Get the image and log elements from the DOM
        const imageElement = document.getElementById('htmlhandwritten');
        const logElement = document.getElementById('log');
        
        // OCR engine selection elements
        const ocrEngineRadios = document.querySelectorAll('input[name="ocr-engine"]');
        const openaiConfig = document.getElementById('openai-config');
        const openaiApiKey = document.getElementById('openai-api-key');
        const openaiEndpoint = document.getElementById('openai-endpoint');
        const claudeConfig = document.getElementById('claude-config');
        const claudeApiKey = document.getElementById('claude-api-key');
        const claudeEndpoint = document.getElementById('claude-endpoint');
        
        // Function to get selected OCR engine
        function getSelectedOCREngine() {
          return document.querySelector('input[name="ocr-engine"]:checked').value;
        }
        
        // Function to update UI based on OCR engine selection
        function updateOCREngineUI() {
          const selectedEngine = getSelectedOCREngine();
          
          // Hide all config sections
          openaiConfig.style.display = 'none';
          claudeConfig.style.display = 'none';
          
          // Show the appropriate config section
          if (selectedEngine === 'openai') {
            openaiConfig.style.display = 'block';
          } else if (selectedEngine === 'claude') {
            claudeConfig.style.display = 'block';
          }
        }
        
        // Event listeners for OCR engine selection
        ocrEngineRadios.forEach(radio => {
          radio.addEventListener('change', updateOCREngineUI);
        });
        
        // Function to update the log/status on the page
        function updateLog(status) {
          logElement.innerHTML = `<p>${status}</p>`;
          console.log(status);
        }
        
        // Function to convert canvas to base64
        function canvasToBase64(canvas) {
          return canvas.toDataURL('image/jpeg', 0.8);
        }
        
        // Claude 3 Opus OCR function
        async function performClaudeOCR(imageData) {
          const apiKey = claudeApiKey.value.trim();
          let endpoint = claudeEndpoint.value.trim() || 'https://api.anthropic.com';
          
          if (!apiKey) {
            throw new Error('Please provide your Anthropic API key for Claude OCR');
          }
          
          updateLog('Performing OCR with Claude 3 Opus...');
          
          const prompt = `Please perform OCR (Optical Character Recognition) on this image and extract all the text content. Return only the extracted text without any additional formatting or explanations.`;
          
          const requestBody = {
            model: "claude-3-5-sonnet-20241022",
            max_tokens: 1000,
            messages: [
              {
                role: "user",
                content: [
                  {
                    type: "text",
                    text: prompt
                  },
                  {
                    type: "image",
                    source: {
                      type: "base64",
                      media_type: "image/jpeg",
                      data: imageData.split(',')[1] // Remove data:image/jpeg;base64, prefix
                    }
                  }
                ]
              }
            ]
          };
          
          try {
            // Try multiple approaches to handle CORS
            const approaches = [
              // Approach 1: Direct API call (may fail due to CORS)
              async () => {
                updateLog('Trying direct API call...');
                const response = await fetch(`${endpoint}/v1/messages`, {
                  method: 'POST',
                  headers: {
                    'x-api-key': apiKey,
                    'anthropic-version': '2023-06-01',
                    'Content-Type': 'application/json'
                  },
                  body: JSON.stringify(requestBody)
                });
                return response;
              },
              // Approach 2: Digital Ocean Functions proxy
              async () => {
                updateLog('Trying Digital Ocean Functions proxy...');
                const response = await fetch('https://faas-sfo3-7872a1dd.doserverless.co/api/v1/web/fn-00585b33-7c19-4732-b11d-1ca044549063/default/ocr-proxy', {
                  method: 'POST',
                  headers: {
                    'Content-Type': 'application/json'
                  },
                  body: JSON.stringify({
                    method: 'POST',
                    path: 'messages',
                    headers: {
                      'x-api-key': apiKey,
                      'anthropic-version': '2023-06-01'
                    },
                    body: JSON.stringify(requestBody),
                    api_type: 'anthropic'
                  })
                });
                return response;
              },
              // Approach 3: CORS proxy (fallback)
              async () => {
                updateLog('Trying CORS proxy...');
                const response = await fetch('https://cors-anywhere.herokuapp.com/https://api.anthropic.com/v1/messages', {
                  method: 'POST',
                  headers: {
                    'x-api-key': apiKey,
                    'anthropic-version': '2023-06-01',
                    'Content-Type': 'application/json',
                    'Origin': window.location.origin
                  },
                  body: JSON.stringify(requestBody)
                });
                return response;
              }
            ];
            
            let lastError;
            
            for (let i = 0; i < approaches.length; i++) {
              try {
                const response = await approaches[i]();
                
                if (!response.ok) {
                  const errorText = await response.text();
                  lastError = `Approach ${i + 1} failed: ${response.status} - ${errorText}`;
                  console.log(lastError);
                  continue;
                }
                
                const result = await response.json();
                
                if (result.content && result.content[0] && result.content[0].text) {
                  return result.content[0].text.trim();
                } else {
                  throw new Error('Unexpected response format from Claude API');
                }
                
              } catch (error) {
                lastError = `Approach ${i + 1} error: ${error.message}`;
                console.log(lastError);
                continue;
              }
            }
            
            // If all approaches failed, provide helpful guidance
            if (lastError && lastError.includes('credit balance is too low')) {
              throw new Error(`Anthropic API requires credits. Your account has insufficient credits.\n\nSolutions:\n1. Add credits to your Anthropic account\n2. Use Tesseract.js instead (works offline, no credits needed)\n3. Try a different OCR service\n\nError: ${lastError}`);
            } else {
              throw new Error(`All API approaches failed. Consider:\n1. Check your Anthropic API key is valid\n2. Ensure the proxy server is running (npm start)\n3. Use Tesseract.js instead (works offline)\n\nLast error: ${lastError}`);
            }
            
          } catch (error) {
            console.error('Claude API error:', error);
            throw error;
          }
        }
        
        // OpenAI GPT-4 Vision OCR function
        async function performOpenAIOCR(imageData) {
          const apiKey = openaiApiKey.value.trim();
          
          updateLog('Performing OCR with OpenAI GPT-4 Vision...');
          
          const prompt = `Please perform OCR (Optical Character Recognition) on this image and extract all the text content. Return only the extracted text without any additional formatting or explanations.`;
          
          const requestBody = {
            model: "gpt-4o",
            messages: [
              {
                role: "user",
                content: [
                  {
                    type: "text",
                    text: prompt
                  },
                  {
                    type: "image_url",
                    image_url: {
                      url: imageData
                    }
                  }
                ]
              }
            ],
            max_tokens: 1000,
            temperature: 0.1
          };
          
          try {
            // Use Digital Ocean Functions proxy
            const response = await fetch('https://faas-sfo3-7872a1dd.doserverless.co/api/v1/web/fn-00585b33-7c19-4732-b11d-1ca044549063/default/ocr-proxy', {
              method: 'POST',
              headers: {
                'Content-Type': 'application/json'
              },
              body: JSON.stringify({
                method: 'POST',
                path: 'chat/completions',
                headers: {
                  'authorization': apiKey ? `Bearer ${apiKey}` : null
                },
                body: JSON.stringify(requestBody),
                api_type: 'openai'
              })
            });
            
            if (!response.ok) {
              const errorText = await response.text();
              throw new Error(`OpenAI API error: ${response.status} - ${errorText}`);
            }
            
            const result = await response.json();
            console.log('Proxy response:', result);
            
            // Check if the response is a direct OpenAI response or wrapped in a proxy response
            if (result.choices && result.choices[0] && result.choices[0].message) {
              // Direct OpenAI response
              console.log('Direct OpenAI response detected');
              return result.choices[0].message.content.trim();
            } else if (result.statusCode === 200 && result.body) {
              // Wrapped proxy response
              try {
                let responseBody;
                if (typeof result.body === 'string') {
                  responseBody = JSON.parse(result.body);
                } else {
                  responseBody = result.body;
                }
                console.log('Parsed response body:', responseBody);
                if (responseBody.choices && responseBody.choices[0] && responseBody.choices[0].message) {
                  return responseBody.choices[0].message.content.trim();
                } else {
                  throw new Error('Unexpected response format from OpenAI API');
                }
              } catch (parseError) {
                console.error('Failed to parse response body:', result.body);
                console.error('Parse error:', parseError);
                throw new Error(`Failed to parse OpenAI response: ${parseError.message}`);
              }
            } else if (result.statusCode && result.statusCode !== 200) {
              // Error response
              try {
                const errorBody = typeof result.body === 'string' ? JSON.parse(result.body) : result.body;
                throw new Error(`OpenAI API error: ${result.statusCode} - ${errorBody.error || errorBody}`);
              } catch (parseError) {
                console.error('Failed to parse error body:', result.body);
                throw new Error(`OpenAI API error: ${result.statusCode} - ${result.body}`);
              }
            } else {
              throw new Error('Unexpected response format from proxy');
            }
            
          } catch (error) {
            console.error('OpenAI API error:', error);
            throw error;
          }
        }
        

        
        // Camera functionality
        const videoElement = document.getElementById('camera-video');
        const startButton = document.getElementById('start-camera');
        const stopButton = document.getElementById('stop-camera');
        const ocrButton = document.getElementById('ocr-camera');
        let stream = null;
        
        // Start camera function
        async function startCamera() {
          try {
            updateLog('Requesting camera permissions...');
            
            // Request camera access with specific constraints for mobile
            stream = await navigator.mediaDevices.getUserMedia({
              video: {
                facingMode: 'environment', // Use back camera on mobile
                width: { ideal: 1280 },
                height: { ideal: 720 }
              },
              audio: false
            });
            
            // Set the video source
            videoElement.srcObject = stream;
            
            updateLog('Camera started successfully!');
            startButton.style.display = 'none';
            stopButton.style.display = 'inline-block';
            
          } catch (error) {
            console.error('Camera access failed:', error);
            updateLog(`Camera access failed: ${error.message}`);
          }
        }
        
        // Stop camera function
        function stopCamera() {
          if (stream) {
            stream.getTracks().forEach(track => track.stop());
            stream = null;
            videoElement.srcObject = null;
            updateLog('Camera stopped');
            startButton.style.display = 'inline-block';
            stopButton.style.display = 'none';
          }
        }
        
        // OCR function for camera video frame
        async function performOCROnCamera() {
          if (!stream || !videoElement.srcObject) {
            updateLog('Please start the camera first');
            return;
          }
          
          const selectedEngine = getSelectedOCREngine();
          
          // Check credentials for AI engines
          if (selectedEngine === 'openai') {
            const apiKey = openaiApiKey.value.trim();
            if (!apiKey) {
              updateLog('OpenAI selected but API key not configured. Please enter your OpenAI API key, or switch to Tesseract.');
              return;
            }
          } else if (selectedEngine === 'claude') {
            const apiKey = claudeApiKey.value.trim();
            if (!apiKey) {
              updateLog('Claude selected but API key not configured. Please enter your Anthropic API key, or switch to Tesseract.');
              return;
            }
          }
          
          updateLog(`Performing OCR on camera frame using ${selectedEngine}...`);
          
          try {
            // Get the display canvas for showing the captured frame
            const displayCanvas = document.getElementById('captured-frame');
            const displayCtx = displayCanvas.getContext('2d');
            
            // Set display canvas size to match video dimensions
            displayCanvas.width = videoElement.videoWidth;
            displayCanvas.height = videoElement.videoHeight;
            
            // Draw the current video frame to the display canvas
            displayCtx.drawImage(videoElement, 0, 0, displayCanvas.width, displayCanvas.height);
            
            // Create a temporary canvas for OCR processing
            const canvas = document.createElement('canvas');
            const ctx = canvas.getContext('2d');
            
            // Set canvas size to match video dimensions
            canvas.width = videoElement.videoWidth;
            canvas.height = videoElement.videoHeight;
            
            // Draw the current video frame to the canvas
            ctx.drawImage(videoElement, 0, 0, canvas.width, canvas.height);
            
            // Store the captured frame for retry functionality
            lastCapturedFrame = canvasToBase64(canvas);
            
            // Update the captured frame display
            const cameraDisplayCanvas = document.getElementById('captured-frame');
            const cameraDisplayCtx = cameraDisplayCanvas.getContext('2d');
            cameraDisplayCanvas.width = canvas.width;
            cameraDisplayCanvas.height = canvas.height;
            cameraDisplayCtx.drawImage(canvas, 0, 0);
            
            updateLog('üì∏ New snapshot captured from camera');
            updateLog('üîç Click "Try OCR text recognition" to extract text');
            
            if (selectedEngine === 'openai') {
              // Use OpenAI for camera OCR
              const imageData = canvasToBase64(canvas);
              const text = await performOpenAIOCR(imageData);
              
              // Display the results
              updateLog('Camera OpenAI OCR Complete!');
              logElement.innerHTML += `<h2>Camera Frame Text:</h2><pre>${text}</pre>`;
              
            } else if (selectedEngine === 'claude') {
              // Use Claude for camera OCR
              const imageData = canvasToBase64(canvas);
              const text = await performClaudeOCR(imageData);
              
              // Display the results
              updateLog('Camera Claude OCR Complete!');
              logElement.innerHTML += `<h2>Camera Frame Text:</h2><pre>${text}</pre>`;
              
            } else {
              // Use Tesseract for camera OCR
              const result = await tesseractJs.recognize(
                canvas,
                'eng',
                {
                  logger: m => {
                    const progressStatus = `Camera OCR: ${m.status} (${(m.progress * 100).toFixed(2)}%)`;
                    updateLog(progressStatus);
                  }
                }
              );
              
              // Debug: Log the full result structure
              console.log('Camera OCR result:', result);
              
              const { data: { text, words } } = result;
              
              // Display the results
              updateLog('Camera Tesseract OCR Complete!');
              logElement.innerHTML += `<h2>Camera Frame Text:</h2><pre>${text}</pre>`;
              
              // Display individual words and their confidence levels (if available)
              if (words && Array.isArray(words)) {
                logElement.innerHTML += '<h2>Camera Frame Words:</h2>';
                const wordList = words.map(w => `<li>${w.text} (Confidence: ${w.confidence.toFixed(2)}%)</li>`).join('');
                logElement.innerHTML += `<ul>${wordList}</ul>`;
              } else {
                logElement.innerHTML += '<p><em>Word-level data not available</em></p>';
              }
            }
            
          } catch (error) {
            console.error('Camera OCR failed:', error);
            updateLog(`Camera OCR failed: ${error.message}`);
          }
        }
        
        // Global variable to store the last captured frame
        let lastCapturedFrame = null;
        
        // Retry OCR function using the last captured frame
        async function retryOCR() {
          console.log('Retry OCR called');
          
          // If no lastCapturedFrame, use the default image
          let imageToProcess;
          let isDefaultImage = false;
          
          if (!lastCapturedFrame) {
            // Use the default image if no camera snapshot has been taken
            imageToProcess = imageElement;
            isDefaultImage = true;
            updateLog('Using default image (knock knock joke) for OCR...');
            console.log('Using default image:', imageElement);
          } else {
            // Use the last captured frame
            imageToProcess = lastCapturedFrame;
            updateLog('Using last captured frame for OCR...');
            console.log('Using last captured frame');
          }
          
          const selectedEngine = getSelectedOCREngine();
          
          // Check credentials for AI engines
          if (selectedEngine === 'openai') {
            const apiKey = openaiApiKey.value.trim();
            if (!apiKey) {
              updateLog('OpenAI selected but no API key provided in UI. Trying to use environment variable from Digital Ocean Function...');
              // Continue without API key - the function will try to use environment variable
            }
          } else if (selectedEngine === 'claude') {
            const apiKey = claudeApiKey.value.trim();
            if (!apiKey) {
              updateLog('Claude selected but API key not configured. Please enter your Anthropic API key, or switch to Tesseract.');
              return;
            }
          }
          
          updateLog(`Performing OCR using ${selectedEngine}...`);
          console.log('Selected engine:', selectedEngine);
          
          try {
            if (selectedEngine === 'openai') {
              // For OpenAI, we need to convert the image to base64
              let imageData;
              if (isDefaultImage) {
                const canvas = document.createElement('canvas');
                const ctx = canvas.getContext('2d');
                canvas.width = imageToProcess.naturalWidth;
                canvas.height = imageToProcess.naturalHeight;
                ctx.drawImage(imageToProcess, 0, 0);
                imageData = canvasToBase64(canvas);
              } else {
                imageData = imageToProcess;
              }
              
              const text = await performOpenAIOCR(imageData);
              
              // Display the results
              updateLog('OpenAI OCR Complete!');
              logElement.innerHTML += `<h2>Extracted Text:</h2><pre>${text}</pre>`;
              
            } else if (selectedEngine === 'claude') {
              // For Claude, we need to convert the image to base64
              let imageData;
              if (isDefaultImage) {
                const canvas = document.createElement('canvas');
                const ctx = canvas.getContext('2d');
                canvas.width = imageToProcess.naturalWidth;
                canvas.height = imageToProcess.naturalHeight;
                ctx.drawImage(imageToProcess, 0, 0);
                imageData = canvasToBase64(canvas);
              } else {
                imageData = imageToProcess;
              }
              
              const text = await performClaudeOCR(imageData);
              
              // Display the results
              updateLog('Claude OCR Complete!');
              logElement.innerHTML += `<h2>Extracted Text:</h2><pre>${text}</pre>`;
              
            } else if (selectedEngine === 'tesseract') {
              // Use Tesseract for OCR
              console.log('Using Tesseract for OCR with image:', imageToProcess);
              updateLog('Note: Tesseract.js may download language models on first use (this is normal)');
              const result = await tesseractJs.recognize(
                imageToProcess,
                'eng',
                {
                  logger: m => {
                    const progressStatus = `OCR: ${m.status} (${(m.progress * 100).toFixed(2)}%)`;
                    updateLog(progressStatus);
                  }
                }
              );
              
              const { data: { text, words } } = result;
              
              // Display the results
              updateLog('Tesseract OCR Complete!');
              logElement.innerHTML += `<h2>Extracted Text:</h2><pre>${text}</pre>`;
              
              // Display individual words and their confidence levels (if available)
              if (words && Array.isArray(words)) {
                logElement.innerHTML += `<h3>Individual Words:</h3><ul>`;
                words.forEach(word => {
                  logElement.innerHTML += `<li>"${word.text}" (confidence: ${(word.confidence * 100).toFixed(2)}%)</li>`;
                });
                logElement.innerHTML += `</ul>`;
              }
            } else {
              updateLog(`Unknown OCR engine: ${selectedEngine}. Please select Tesseract, Claude, or OpenAI.`);
            }
          } catch (error) {
            updateLog(`OCR failed: ${error.message}`);
            console.error('OCR error details:', error);
            if (error.message.includes('NetworkError')) {
              updateLog('This may be due to network issues or firewall blocking Tesseract model downloads.');
              updateLog('Try refreshing the page or check your internet connection.');
            }
          }
        }
        
        // Event listeners for camera buttons
        startButton.addEventListener('click', startCamera);
        stopButton.addEventListener('click', stopCamera);
        ocrButton.addEventListener('click', performOCROnCamera);
        
        // Add event listener for retry button
        const retryButton = document.getElementById('retry-ocr');
        retryButton.addEventListener('click', retryOCR);
        
        // Test API connection function
        async function testAPIConnection() {
          const apiKey = claudeApiKey.value.trim();
          if (!apiKey) {
            updateLog('Please enter your Anthropic API key first');
            return;
          }
          
          updateLog('Testing API connection...');
          
          try {
            const response = await fetch('https://faas-sfo3-7872a1dd.doserverless.co/api/v1/web/fn-00585b33-7c19-4732-b11d-1ca044549063/default/ocr-proxy', {
              method: 'POST',
              headers: {
                'Content-Type': 'application/json'
              },
              body: JSON.stringify({
                method: 'POST',
                path: 'test-anthropic',
                headers: {
                  'x-api-key': apiKey
                },
                api_type: 'anthropic'
              })
            });
            
            const result = await response.json();
            
            if (result.statusCode === 200) {
              const body = JSON.parse(result.body);
              if (body.status === 400 && body.response && body.response.includes('credit balance is too low')) {
                updateLog('‚ùå API Test Failed: Insufficient credits');
                updateLog('Your Anthropic account needs more credits to use the API.');
                updateLog('');
                updateLog('Troubleshooting:');
                updateLog('1. Check your Anthropic billing dashboard');
                updateLog('2. Credits may take 5-10 minutes to process');
                updateLog('3. Ensure you added credits to the correct account');
                updateLog('4. Try refreshing your API key');
                updateLog('');
                updateLog('Alternative Solutions:');
                updateLog('‚úÖ Use Tesseract.js (works offline, no credits needed)');
                updateLog('üîß Try a different OCR service');
                updateLog('‚è∞ Wait 10 minutes and test again');
              } else if (body.status === 200) {
                updateLog('‚úÖ API Test Successful: Your API key is working!');
              } else {
                updateLog(`‚ùå API Test Failed: ${body.status} - ${body.response || body.error}`);
              }
            } else {
              updateLog(`‚ùå Proxy Test Failed: ${result.statusCode} - ${result.body}`);
            }
          } catch (error) {
            updateLog(`‚ùå API Test Failed: ${error.message}`);
            updateLog('Make sure the Digital Ocean Functions proxy is accessible');
          }
        }
        
        // Add test button to Claude config
        const testButton = document.createElement('button');
        testButton.textContent = 'Test API Connection';
        testButton.onclick = testAPIConnection;
        testButton.style.marginTop = '10px';
        claudeConfig.appendChild(testButton);
        
        // Add switch to Tesseract button
        const switchButton = document.createElement('button');
        switchButton.textContent = 'Switch to Tesseract (Free)';
        switchButton.onclick = () => {
          document.querySelector('input[value="tesseract"]').checked = true;
          updateOCREngineUI();
          updateLog('‚úÖ Switched to Tesseract.js - No credits needed!');
        };
        switchButton.style.marginTop = '5px';
        switchButton.style.marginLeft = '10px';
        switchButton.style.backgroundColor = '#4CAF50';
        switchButton.style.color = 'white';
        switchButton.style.border = 'none';
        switchButton.style.padding = '5px 10px';
        switchButton.style.borderRadius = '3px';
        switchButton.style.cursor = 'pointer';
        claudeConfig.appendChild(switchButton);
        
        // Initially hide the stop button
        stopButton.style.display = 'none';
        
        // Initialize OCR engine UI
        updateOCREngineUI();
        
        // Initialize with default image OCR
        async function initializeWithDefaultImage() {
          updateLog('Setting up default image (knock knock joke) for OCR...');
          
          // Ensure Tesseract is selected by default
          document.querySelector('input[value="tesseract"]').checked = true;
          updateOCREngineUI();
          
          try {
            // Check if image is properly loaded
            if (!imageElement.complete || imageElement.naturalWidth === 0) {
              updateLog('Waiting for image to load completely...');
              await new Promise((resolve) => {
                imageElement.onload = resolve;
                imageElement.onerror = () => {
                  throw new Error('Failed to load default image');
                };
              });
            }
            
            // Create a canvas to draw the default image
            const canvas = document.createElement('canvas');
            const ctx = canvas.getContext('2d');
            
            // Set canvas size to match the image
            canvas.width = imageElement.naturalWidth;
            canvas.height = imageElement.naturalHeight;
            
            // Draw the default image to the canvas
            ctx.drawImage(imageElement, 0, 0);
            
            // Store as the last captured frame
            lastCapturedFrame = canvasToBase64(canvas);
            
            // Update the captured frame display to show the default image
            const displayCanvas = document.getElementById('captured-frame');
            const displayCtx = displayCanvas.getContext('2d');
            displayCanvas.width = canvas.width;
            displayCanvas.height = canvas.height;
            displayCtx.drawImage(canvas, 0, 0);
            
            updateLog('‚úÖ Default image loaded and ready for OCR');
            updateLog('üì∏ Current snapshot: Knock knock joke image');
            updateLog('üîç Click "Try OCR text recognition" to extract text');
            
            // Try to perform initial OCR, but don't fail if there are security issues
            try {
              updateLog('Performing OCR on default image using Tesseract.js...');
              
              // Use Tesseract for default image OCR
              updateLog('Note: Tesseract.js may download language models on first use (this is normal)');
              const result = await tesseractJs.recognize(
                canvas,
                'eng',
                {
                  logger: m => {
                    const progressStatus = `Default Image OCR: ${m.status} (${(m.progress * 100).toFixed(2)}%)`;
                    updateLog(progressStatus);
                  }
                }
              );
              
              const { data: { text, words } } = result;
              
              updateLog('Default image Tesseract OCR Complete!');
              logElement.innerHTML += `<h2>Default Image Text:</h2><pre>${text}</pre>`;
              
              // Display individual words and their confidence levels (if available)
              if (words && Array.isArray(words)) {
                logElement.innerHTML += `<h3>Individual Words:</h3><ul>`;
                words.forEach(word => {
                  logElement.innerHTML += `<li>"${word.text}" (confidence: ${(word.confidence * 100).toFixed(2)}%)</li>`;
                });
                logElement.innerHTML += `</ul>`;
              }
            } catch (ocrError) {
              updateLog(`Initial OCR failed: ${ocrError.message}`);
              updateLog('You can still use the "Try OCR text recognition" button to perform OCR manually.');
            }
          } catch (error) {
            updateLog(`Default image OCR failed: ${error.message}`);
            if (error.message.includes('NetworkError')) {
              updateLog('This may be due to network issues or firewall blocking Tesseract model downloads.');
              updateLog('Try refreshing the page or check your internet connection.');
            } else if (error.message.includes('insecure')) {
              updateLog('This error is likely due to CORS restrictions when loading from a file:// URL.');
              updateLog('Please serve the page from a web server (e.g., python3 -m http.server) instead of opening the file directly.');
              updateLog('The OCR will work once the page is served from a proper web server.');
            }
          }
        }
        
        // Wait for the image to load, then initialize
        if (imageElement.complete) {
          initializeWithDefaultImage();
        } else {
          imageElement.addEventListener('load', initializeWithDefaultImage);
        }
    </script>

</html>