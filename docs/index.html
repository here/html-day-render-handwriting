<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Hand-written HTML to Text</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/6.65.7/codemirror.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/6.65.7/theme/solarized.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/6.65.7/codemirror.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/6.65.7/mode/javascript/javascript.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/6.65.7/mode/xml/xml.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/6.65.7/mode/css/css.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/6.65.7/mode/htmlmixed/htmlmixed.min.js"></script>
</head>
<body>
  <style>
    body {
      text-align: center;
      font-family: Arial, sans-serif;
      margin: 0 auto;
      padding: 20px;
      position: relative;
      overflow-x: hidden;
    }
    
    body::before {
      content: '';
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      background: repeating-linear-gradient(in hsl longer hue, rgba(255,0,0,0.1), rgba(0,0,255,0.1) 25px);
      background-size: 50px 50px;
      animation: moveLines 8s linear infinite;
      animation-play-state: running;
      will-change: transform;
      transform: translateZ(0);
      z-index: -1;
      pointer-events: none;
    }
    
    @keyframes moveLines {
      0% {
        background-position: 0px 0px;
      }
      100% {
        background-position: 40px 0px;
      }
    }
    
    .camera-row {
      display: flex;
      justify-content: center;
      align-items: flex-start;
      gap: 20px;
      margin: 20px 0;
      flex-wrap: wrap;
    }
    
    .camera-section, .frame-section {
      flex: 1;
      min-width: 300px;
      max-width: 400px;
    }
    
    /* Stack vertically on smaller screens */
    @media (max-width: 768px) {
      .camera-row {
        flex-direction: column;
        align-items: center;
      }
      
      .camera-section, .frame-section {
        min-width: 250px;
        max-width: 100%;
      }
      
      #camera-video, #captured-frame {
        width: 100%;
        max-width: 400px;
        height: 150px;
      }
    }
    
    /* Even smaller screens */
    @media (max-width: 480px) {
      .camera-section, .frame-section {
        min-width: 200px;
      }
      
      #camera-video, #captured-frame {
        height: 120px;
      }
    }
    
    .camera-section h3, .frame-section h3 {
      margin-top: 0;
      margin-bottom: 10px;
    }
    
    #camera-video, #captured-frame {
      width: auto;
      height: 200px;
      border: 2px solid #ccc;
      border-radius: 8px;
    }
    
    button {
      margin: 5px;
      padding: 8px 16px;
      border: none;
      border-radius: 4px;
      background-color: #007bff;
      color: white;
    }
    
    button:hover {
      background-color: #0056b3;
    }
    
    #ocr-engine-selection {
      margin: 20px 0;
      text-align: left;
      max-width: 600px;
      margin-left: auto;
      margin-right: auto;
    }
    
    #log {
      margin: 20px 0;
      text-align: left;
      max-width: 800px;
      margin-left: auto;
      margin-right: auto;
      padding: 15px;
      background-color: #f8f9fa;
      border-radius: 8px;
      border: 1px solid #dee2e6;
    }
    
    /* CodeMirror customizations */
    .CodeMirror {
      height: auto;
      min-height: 300px;
      max-height: 600px;
      font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
      font-size: 14px;
      line-height: 1.4;
      resize: vertical;
      overflow: hidden;
      text-align: left;
    }
    
    .CodeMirror-lines {
      text-align: left;
    }
    
    .CodeMirror-code {
      text-align: left;
    }
    
    .CodeMirror-scroll {
      min-height: 300px;
      max-height: 600px;
      overflow: hidden;
    }
    
    /* Hide scroll bars */
    .CodeMirror-scrollbar-filler,
    .CodeMirror-gutter-filler {
      display: none;
    }
    
    .CodeMirror-simplescroll-horizontal div,
    .CodeMirror-simplescroll-vertical div {
      display: none;
    }
    
    #codemirror-container {
      max-width: 800px;
      margin: 0 auto;
      resize: vertical;
      overflow: hidden;
    }
    
    #codemirror-container {
      max-width: 800px;
      margin: 0 auto;
    }
    
    input[type="text"], input[type="password"] {
      width: 100%;
      padding: 8px;
      margin: 5px 0;
      border: 1px solid #ccc;
      border-radius: 4px;
    }
    
    label {
      display: block;
      margin: 10px 0 5px 0;
      font-weight: bold;
    }
  </style>

  <h2>hand-written html to text</h2>
  <p>This demo for <a href="https://htmlday.com/2025">html day 2025</a> attempts to read html and render it on the web.</p>
  <p>There are two options for <a href="https://en.wikipedia.org/wiki/Optical_character_recognition">optical character recognition (OCR)</a>.</p>
  <p>TesseractJS is the default and runs entirely on your local browser, while OpenAI will use an 3rd party AI model.</p>
  
  <!-- Camera and Frame Row -->
  <div class="camera-row">
    <div class="camera-section">
      <h3>Camera View:</h3>
      <video id="camera-video" autoplay playsinline muted></video>
    </div>
    
    <div class="frame-section">
      <h3>Last Captured Frame:</h3>
      <canvas id="captured-frame"></canvas>
    </div>
  </div>

  <div>
    <button id="start-camera">Start Camera</button>
    <button id="stop-camera">Stop Camera</button>
    <button id="ocr-camera">Take new snapshot from camera</button>
    <button id="retry-ocr">Try OCR text recognition with current snapshot</button>
  </div>
  
          <div id="log"></div>

      
      <div id="text-editor-container" style="margin: 20px 0;">
        <h3>Extracted Text (Editable):</h3>
        <div id="codemirror-container" style="border: 2px solid #ccc; border-radius: 4px; min-height: 200px;"></div>
      </div>

      <button id="open-text-tab" style="background-color: #28a745;">Open Text in New Tab</button>
      <button id="update-preview" style="background-color: #007bff;">Update Preview</button>

  <!-- OCR Engine Selection -->
  <div id="ocr-engine-selection">
    <h3>Optical character recognition strategy</h3>
    <label>
      <input type="radio" name="ocr-engine" value="tesseract" checked> Tesseract.js - Privately runs in browser with mixed accuracy.
    </label>
    <label>
      <input type="radio" name="ocr-engine" value="openai"> OpenAI GPT-4 Vision - 3rd party AI with high accuracy.
    </label>



  </div>
  
      <div style="margin: 30px 0;">
      <h3>Preview</h3>
      <iframe id="preview-iframe" width="100%" height="400px" style="border: 2px solid #ccc; border-radius: 8px; background: white; max-width: 800px; margin: 0 auto; display: block;"></iframe>
    </div>


  <p><a href="https://github.com/naptha/tesseract.js">tesseract.js</a> to perform OCR on the camera frame.</p>

  <p>Code is at <a href="https://github.com/here/html-day-render-handwriting">github/here/html-day-render-handwriting</a></p>
 
    <script type="module">
        import tesseractJs from "https://esm.sh/tesseract.js@6.0.1";

        // Get the log element from the DOM
        const logElement = document.getElementById('log');
        const previewIframe = document.getElementById('preview-iframe');
        
        // Initialize CodeMirror editor
        let codeMirrorEditor;
        document.addEventListener('DOMContentLoaded', function() {
          codeMirrorEditor = CodeMirror(document.getElementById('codemirror-container'), {
            mode: 'htmlmixed',
            theme: 'solarized light',
            lineNumbers: true,
            autoCloseBrackets: true,
            matchBrackets: true,
            indentUnit: 2,
            tabSize: 2,
            indentWithTabs: false,
            lineWrapping: true,
            placeholder: 'OCR results will appear here...',
            value: '<p>Howdy </p>\n<p>There </p>',
            scrollbarStyle: 'null',
            lineWiseCopyCut: false
          });
          
          // Set default height to 15 lines (approximately 15 * 1.4 * 14px = 294px)
          codeMirrorEditor.setSize(null, '294px');
          
          // Make the editor resizable
          codeMirrorEditor.refresh();
        });
        
        // OCR engine selection elements
        const ocrEngineRadios = document.querySelectorAll('input[name="ocr-engine"]');
        
        // Function to get selected OCR engine
        function getSelectedOCREngine() {
          const selectedRadio = document.querySelector('input[name="ocr-engine"]:checked');
          if (!selectedRadio) {
            // Default to tesseract if no radio button is selected
            const tesseractRadio = document.querySelector('input[value="tesseract"]');
            if (tesseractRadio) {
              tesseractRadio.checked = true;
              return 'tesseract';
            }
            return 'tesseract'; // fallback
          }
          return selectedRadio.value;
        }
        
        // Function to update UI based on OCR engine selection
        function updateOCREngineUI() {
          const selectedEngine = getSelectedOCREngine();
          // No configuration needed - both engines work without UI configuration
        }
        
        // Event listeners for OCR engine selection
        ocrEngineRadios.forEach(radio => {
          radio.addEventListener('change', updateOCREngineUI);
        });
        
        // Function to update the log/status on the page
        function updateLog(status) {
          logElement.innerHTML = `<p>${status}</p>`;
          console.log(status);
        }
        
        // Function to convert canvas to base64
        function canvasToBase64(canvas) {
          return canvas.toDataURL('image/jpeg', 0.8);
        }
        

        
        // OpenAI GPT-4 Vision OCR function
        async function performOpenAIOCR(imageData) {
          updateLog('Performing OCR with OpenAI GPT-4 Vision...');
          
          const prompt = `Please perform OCR (Optical Character Recognition) on this image and extract all the text content. Return only the extracted text without any additional formatting or explanations.`;
          
          const requestBody = {
            model: "gpt-4o",
            messages: [
              {
                role: "user",
                content: [
                  {
                    type: "text",
                    text: prompt
                  },
                  {
                    type: "image_url",
                    image_url: {
                      url: imageData
                    }
                  }
                ]
              }
            ],
            max_tokens: 1000,
            temperature: 0.1
          };
          
          try {
            // Use Digital Ocean Functions proxy
            const response = await fetch('https://faas-sfo3-7872a1dd.doserverless.co/api/v1/web/fn-00585b33-7c19-4732-b11d-1ca044549063/default/ocr-proxy', {
              method: 'POST',
              headers: {
                'Content-Type': 'application/json'
              },
              body: JSON.stringify({
                method: 'POST',
                path: 'chat/completions',
                headers: {
                  'authorization': null
                },
                body: JSON.stringify(requestBody),
                api_type: 'openai'
              })
            });
            
            if (!response.ok) {
              const errorText = await response.text();
              throw new Error(`OpenAI API error: ${response.status} - ${errorText}`);
            }
            
            const result = await response.json();
            console.log('Proxy response:', result);
            
            // Check if the response is a direct OpenAI response or wrapped in a proxy response
            if (result.choices && result.choices[0] && result.choices[0].message) {
              // Direct OpenAI response
              console.log('Direct OpenAI response detected');
              return result.choices[0].message.content.trim();
            } else if (result.statusCode === 200 && result.body) {
              // Wrapped proxy response
              try {
                let responseBody;
                if (typeof result.body === 'string') {
                  responseBody = JSON.parse(result.body);
                } else {
                  responseBody = result.body;
                }
                console.log('Parsed response body:', responseBody);
                if (responseBody.choices && responseBody.choices[0] && responseBody.choices[0].message) {
                  return responseBody.choices[0].message.content.trim();
                } else {
                  throw new Error('Unexpected response format from OpenAI API');
                }
              } catch (parseError) {
                console.error('Failed to parse response body:', result.body);
                console.error('Parse error:', parseError);
                throw new Error(`Failed to parse OpenAI response: ${parseError.message}`);
              }
            } else if (result.statusCode && result.statusCode !== 200) {
              // Error response
              try {
                const errorBody = typeof result.body === 'string' ? JSON.parse(result.body) : result.body;
                throw new Error(`OpenAI API error: ${result.statusCode} - ${errorBody.error || errorBody}`);
              } catch (parseError) {
                console.error('Failed to parse error body:', result.body);
                throw new Error(`OpenAI API error: ${result.statusCode} - ${result.body}`);
              }
            } else {
              throw new Error('Unexpected response format from proxy');
            }
            
          } catch (error) {
            console.error('OpenAI API error:', error);
            throw error;
          }
        }
        

        
        // Camera functionality
        const videoElement = document.getElementById('camera-video');
        const startButton = document.getElementById('start-camera');
        const stopButton = document.getElementById('stop-camera');
        const ocrButton = document.getElementById('ocr-camera');
        let stream = null;
        
        // Start camera function
        async function startCamera() {
          try {
            updateLog('Requesting camera permissions...');
            
            // Request camera access with specific constraints for mobile
            stream = await navigator.mediaDevices.getUserMedia({
              video: {
                facingMode: 'environment', // Use back camera on mobile
                width: { ideal: 1280 },
                height: { ideal: 720 }
              },
              audio: false
            });
            
            // Set the video source
            videoElement.srcObject = stream;
            
            updateLog('Camera started successfully!');
            startButton.style.display = 'none';
            stopButton.style.display = 'inline-block';
            
          } catch (error) {
            console.error('Camera access failed:', error);
            updateLog(`Camera access failed: ${error.message}`);
          }
        }
        
        // Stop camera function
        function stopCamera() {
          if (stream) {
            stream.getTracks().forEach(track => track.stop());
            stream = null;
            videoElement.srcObject = null;
            updateLog('Camera stopped');
            startButton.style.display = 'inline-block';
            stopButton.style.display = 'none';
          }
        }
        
        // OCR function for camera video frame
        async function performOCROnCamera() {
          if (!stream || !videoElement.srcObject) {
            updateLog('Please start the camera first');
            return;
          }
          
          const selectedEngine = getSelectedOCREngine();
          

          
          updateLog(`Performing OCR on camera frame using ${selectedEngine}...`);
          
          try {
            // Get the display canvas for showing the captured frame
            const displayCanvas = document.getElementById('captured-frame');
            const displayCtx = displayCanvas.getContext('2d');
            
            // Set display canvas size to match video dimensions
            displayCanvas.width = videoElement.videoWidth;
            displayCanvas.height = videoElement.videoHeight;
            
            // Draw the current video frame to the display canvas
            displayCtx.drawImage(videoElement, 0, 0, displayCanvas.width, displayCanvas.height);
            
            // Create a temporary canvas for OCR processing
            const canvas = document.createElement('canvas');
            const ctx = canvas.getContext('2d');
            
            // Set canvas size to match video dimensions
            canvas.width = videoElement.videoWidth;
            canvas.height = videoElement.videoHeight;
            
            // Draw the current video frame to the canvas
            ctx.drawImage(videoElement, 0, 0, canvas.width, canvas.height);
            
            // Store the captured frame for retry functionality
            lastCapturedFrame = canvasToBase64(canvas);
            
            // Update the captured frame display
            const cameraDisplayCanvas = document.getElementById('captured-frame');
            const cameraDisplayCtx = cameraDisplayCanvas.getContext('2d');
            cameraDisplayCanvas.width = canvas.width;
            cameraDisplayCanvas.height = canvas.height;
            cameraDisplayCtx.drawImage(canvas, 0, 0);
            
            updateLog('üì∏ New snapshot captured from camera');
            updateLog('üîç Click "Try OCR text recognition" to extract text');
            
            if (selectedEngine === 'openai') {
              // Use OpenAI for camera OCR
              const imageData = canvasToBase64(canvas);
              const text = await performOpenAIOCR(imageData);
              
                              // Store the extracted text in CodeMirror
                if (codeMirrorEditor) {
                  codeMirrorEditor.setValue(text);
                }
                
                // Show the buttons
                document.getElementById('open-text-tab').style.display = 'inline-block';
                document.getElementById('update-preview').style.display = 'inline-block';
                
                // Display the results
                updateLog('Camera OpenAI OCR Complete!');
              
                        } else {
              // Use Tesseract for camera OCR
              const result = await tesseractJs.recognize(
                canvas,
                'eng',
                {
                  logger: m => {
                    const progressStatus = `Camera OCR: ${m.status} (${(m.progress * 100).toFixed(2)}%)`;
                    updateLog(progressStatus);
                  }
                }
              );
              
              // Debug: Log the full result structure
              console.log('Camera OCR result:', result);
              
              const { data: { text, words } } = result;
              
              // Store the extracted text in CodeMirror
              if (codeMirrorEditor) {
                codeMirrorEditor.setValue(text);
              }
              
              // Display the results
              updateLog('Camera Tesseract OCR Complete!');
              
              // Display individual words and their confidence levels (if available)
              if (words && Array.isArray(words)) {
                updateLog(`Individual words detected: ${words.length} words`);
                words.forEach(word => {
                  updateLog(`"${word.text}" (confidence: ${(word.confidence * 100).toFixed(2)}%)`);
                });
              } else {
                updateLog('Word-level data not available');
              }
            }
            
          } catch (error) {
            console.error('Camera OCR failed:', error);
            updateLog(`Camera OCR failed: ${error.message}`);
          }
        }
        
        // Global variable to store the last captured frame
        let lastCapturedFrame = null;
        
        // Retry OCR function using the last captured frame
        async function retryOCR() {
          console.log('Retry OCR called');
          
          // Use the last captured frame (which should be the default image if no camera snapshot taken)
          if (!lastCapturedFrame) {
            updateLog('No captured frame available. Please take a camera snapshot first.');
            return;
          }
          
          const imageToProcess = lastCapturedFrame;
          updateLog('Using last captured frame for OCR...');
          console.log('Using last captured frame');
          
          const selectedEngine = getSelectedOCREngine();
          

          
          updateLog(`Performing OCR using ${selectedEngine}...`);
          console.log('Selected engine:', selectedEngine);
          
          try {
            if (selectedEngine === 'openai') {
              // For OpenAI, we need to convert the image to base64
              const text = await performOpenAIOCR(imageToProcess);
              
              // Store the extracted text in CodeMirror
              if (codeMirrorEditor) {
                codeMirrorEditor.setValue(text);
              }
              
              // Display the results
              updateLog('OpenAI OCR Complete!');
              
            } else if (selectedEngine === 'tesseract') {
              // Use Tesseract for OCR
              console.log('Using Tesseract for OCR with image:', imageToProcess);
              updateLog('Note: Tesseract.js may download language models on first use (this is normal)');
              const result = await tesseractJs.recognize(
                imageToProcess,
                'eng',
                {
                  logger: m => {
                    const progressStatus = `OCR: ${m.status} (${(m.progress * 100).toFixed(2)}%)`;
                    updateLog(progressStatus);
                  }
                }
              );
              
              const { data: { text, words } } = result;
              
              // Store the extracted text in CodeMirror
              if (codeMirrorEditor) {
                codeMirrorEditor.setValue(text);
              }
              
              // Display the results
              updateLog('Tesseract OCR Complete!');
              
              // Display individual words and their confidence levels (if available)
              if (words && Array.isArray(words)) {
                updateLog(`Individual words detected: ${words.length} words`);
                words.forEach(word => {
                  updateLog(`"${word.text}" (confidence: ${(word.confidence * 100).toFixed(2)}%)`);
                });
              }
                          } else {
                updateLog(`Unknown OCR engine: ${selectedEngine}. Please select Tesseract or OpenAI.`);
              }
          } catch (error) {
            updateLog(`OCR failed: ${error.message}`);
            console.error('OCR error details:', error);
            if (error.message.includes('NetworkError')) {
              updateLog('This may be due to network issues or firewall blocking Tesseract model downloads.');
              updateLog('Try refreshing the page or check your internet connection.');
            }
          }
        }
        
        // Event listeners for camera buttons
        startButton.addEventListener('click', startCamera);
        stopButton.addEventListener('click', stopCamera);
        ocrButton.addEventListener('click', performOCROnCamera);
        
        // Function to open extracted text in new tab
        function openTextInNewTab() {
          if (!codeMirrorEditor) {
            updateLog('CodeMirror editor not initialized yet. Please wait a moment and try again.');
            return;
          }
          
          const text = codeMirrorEditor.getValue().trim();
          
          if (!text) {
            updateLog('No text available. Please perform OCR first or enter text manually.');
            return;
          }
          
          const newWindow = window.open('', '_blank');
          newWindow.document.write(text);
          newWindow.document.close();
          
          updateLog('‚úÖ Text opened in new tab');
        }
        
        // Function to update preview iframe
        function updatePreview() {
          if (!codeMirrorEditor) {
            updateLog('CodeMirror editor not initialized yet. Please wait a moment and try again.');
            return;
          }
          
          const text = codeMirrorEditor.getValue().trim();
          
          if (!text) {
            updateLog('No text available. Please perform OCR first or enter text manually.');
            return;
          }
          
          try {
            const iframe = document.getElementById('preview-iframe');
            const iframeDoc = iframe.contentDocument || iframe.contentWindow.document;
            iframeDoc.open();
            iframeDoc.write(text);
            iframeDoc.close();
            
            updateLog('‚úÖ Preview updated');
          } catch (error) {
            updateLog('‚ùå Failed to update preview: ' + error.message);
          }
        }
        

        
        // Add event listener for retry button
        const retryButton = document.getElementById('retry-ocr');
        retryButton.addEventListener('click', retryOCR);
        
        // Add event listener for open text tab button
        const openTextTabButton = document.getElementById('open-text-tab');
        openTextTabButton.addEventListener('click', openTextInNewTab);
        
        // Add event listener for update preview button
        const updatePreviewButton = document.getElementById('update-preview');
        updatePreviewButton.addEventListener('click', updatePreview);
        

        

        
        // Initially hide the stop button
        stopButton.style.display = 'none';
        
        // Initialize OCR engine UI
        updateOCREngineUI();
        
        // Initialize with default image and text
        async function initializeWithDefaultImage() {
          try {
            // Create an image element to load the default image
            const defaultImage = new Image();
            defaultImage.crossOrigin = 'anonymous';
            
            defaultImage.onload = function() {
              // Create canvas to draw the default image
              const canvas = document.createElement('canvas');
              const ctx = canvas.getContext('2d');
              
              // Set canvas size to match the image
              canvas.width = defaultImage.naturalWidth;
              canvas.height = defaultImage.naturalHeight;
              
              // Draw the default image to the canvas
              ctx.drawImage(defaultImage, 0, 0);
              
              // Store as the last captured frame
              lastCapturedFrame = canvasToBase64(canvas);
              
              // Update the captured frame display to show the default image
              const displayCanvas = document.getElementById('captured-frame');
              const displayCtx = displayCanvas.getContext('2d');
              displayCanvas.width = canvas.width;
              displayCanvas.height = canvas.height;
              displayCtx.drawImage(canvas, 0, 0);
              
              updateLog('‚úÖ Default image loaded and ready for OCR');
              updateLog('üì∏ Current snapshot: howdy-there-html-day-2025.png');
              updateLog('üîç Click "Try OCR text recognition" to extract text');
            };
            
            defaultImage.onerror = function() {
              updateLog('‚ùå Failed to load default image');
            };
            
            // Load the default image
            defaultImage.src = 'howdy-there-html-day-2025.png';
            

            
          } catch (error) {
            updateLog(`Default image initialization failed: ${error.message}`);
          }
        }
        
        // Initialize with default image and text
        initializeWithDefaultImage();
        
        // Initialize OCR engine UI after DOM is ready
        updateOCREngineUI();
          </script>
  </body>
</html>