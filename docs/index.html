<html>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/6.65.7/codemirror.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/6.65.7/theme/solarized.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/6.65.7/codemirror.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/6.65.7/mode/javascript/javascript.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/6.65.7/mode/xml/xml.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/6.65.7/mode/css/css.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/6.65.7/mode/htmlmixed/htmlmixed.min.js"></script>
  <style>
    body {
      text-align: center;
      font-family: Arial, sans-serif;
      margin: 0 auto;
      padding: 20px;
      position: relative;
      overflow-x: hidden;
    }
    
    body::before {
      content: '';
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      background: repeating-linear-gradient(in hsl longer hue, rgba(255,0,0,0.1), rgba(0,0,255,0.1) 25px);
      background-size: 50px 50px;
      animation: moveLines 8s linear infinite;
      animation-play-state: running;
      will-change: transform;
      transform: translateZ(0);
      z-index: -1;
      pointer-events: none;
    }
    
    @keyframes moveLines {
      0% {
        background-position: 0px 0px;
      }
      100% {
        background-position: 40px 0px;
      }
    }
    
    .camera-row {
      display: flex;
      justify-content: center;
      align-items: flex-start;
      gap: 20px;
      margin: 20px 0;
      flex-wrap: wrap;
    }
    
    .camera-section, .frame-section {
      flex: 1;
      min-width: 300px;
      max-width: 400px;
    }
    
    /* Stack vertically on smaller screens */
    @media (max-width: 768px) {
      .camera-row {
        flex-direction: column;
        align-items: center;
      }
      
      .camera-section, .frame-section {
        min-width: 250px;
        max-width: 100%;
      }
      
      #camera-video, #captured-frame {
        width: 100%;
        max-width: 400px;
        height: 150px;
      }
    }
    
    /* Even smaller screens */
    @media (max-width: 480px) {
      .camera-section, .frame-section {
        min-width: 200px;
      }
      
      #camera-video, #captured-frame {
        height: 120px;
      }
    }
    
    .camera-section h3, .frame-section h3 {
      margin-top: 0;
      margin-bottom: 10px;
    }
    
    #camera-video, #captured-frame {
      width: auto;
      height: 200px;
      border: 2px solid #ccc;
      border-radius: 8px;
    }
    
    button {
      margin: 5px;
      padding: 8px 16px;
      border: none;
      border-radius: 4px;
      background-color: #007bff;
      color: white;
    }
    
    button:hover {
      background-color: #0056b3;
    }
    
    #ocr-engine-selection {
      margin: 20px 0;
      text-align: left;
      max-width: 600px;
      margin-left: auto;
      margin-right: auto;
    }
    
    #log {
      margin: 20px 0;
      text-align: left;
      max-width: 800px;
      margin-left: auto;
      margin-right: auto;
      padding: 15px;
      background-color: #f8f9fa;
      border-radius: 8px;
      border: 1px solid #dee2e6;
    }
    
    /* CodeMirror customizations */
    .CodeMirror {
      height: auto;
      min-height: 300px;
      max-height: 600px;
      font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
      font-size: 14px;
      line-height: 1.4;
      resize: vertical;
      overflow: hidden;
      text-align: left;
    }
    
    .CodeMirror-lines {
      text-align: left;
    }
    
    .CodeMirror-code {
      text-align: left;
    }
    
    .CodeMirror-scroll {
      min-height: 300px;
      max-height: 600px;
      overflow: hidden;
    }
    
    /* Hide scroll bars */
    .CodeMirror-scrollbar-filler,
    .CodeMirror-gutter-filler {
      display: none;
    }
    
    .CodeMirror-simplescroll-horizontal div,
    .CodeMirror-simplescroll-vertical div {
      display: none;
    }
    
    #codemirror-container {
      max-width: 800px;
      margin: 0 auto;
      resize: vertical;
      overflow: hidden;
    }
    
    #codemirror-container {
      max-width: 800px;
      margin: 0 auto;
    }
    
    input[type="text"], input[type="password"] {
      width: 100%;
      padding: 8px;
      margin: 5px 0;
      border: 1px solid #ccc;
      border-radius: 4px;
    }
    
    label {
      display: block;
      margin: 10px 0 5px 0;
      font-weight: bold;
    }
  </style>

  <h2>hand-written html to text</h2>
  <p>This demo for <a href="https://htmlday.com/2025">html day 2025</a> attempts to read html from a device camera and render it on the web.</p>
  <p>Select <a href="https://tesseract.projectnaptha.com/">tesseract.js</a> or OpenAI for <a href="https://en.wikipedia.org/wiki/Optical_character_recognition">optical character recognition (OCR)</a> strategy.

  <p>Code is at <a href="https://github.com/here/html-day-render-handwriting">github/here/html-day-render-handwriting</a></p>
  
  <!-- Camera and Frame Row -->
  <div class="camera-row">
    <div class="camera-section">
      <h3>Camera View:</h3>
      <video id="camera-video" autoplay playsinline muted></video>
    </div>
    
    <div class="frame-section">
      <h3>Last Captured Frame:</h3>
      <canvas id="captured-frame"></canvas>
    </div>
  </div>

  <div>
    <button id="start-camera">Start Camera</button>
    <button id="stop-camera">Stop Camera</button>
    <button id="ocr-camera">Take new snapshot from camera</button>
    <button id="retry-ocr">Try OCR text recognition with current snapshot</button>
  </div>
  
          <div id="log"></div>

      
      <div id="text-editor-container" style="margin: 20px 0;">
        <h3>Extracted Text (Editable):</h3>
        <div id="codemirror-container" style="border: 2px solid #ccc; border-radius: 4px; min-height: 200px;"></div>
      </div>

      <button id="open-text-tab" style="background-color: #28a745; display: none;">Open Text in New Tab</button>

  <!-- OCR Engine Selection -->
  <div id="ocr-engine-selection">
    <h3>Optical character recognition strategy</h3>
    <label>
      <input type="radio" name="ocr-engine" value="tesseract" checked> Tesseract.js - Runs entirely in your browser
    </label>
    <label>
      <input type="radio" name="ocr-engine" value="openai"> OpenAI GPT-4 Vision - Sends images to OpenAI to extract text
    </label>

    <div id="openai-config" style="display: none; margin-top: 10px;">
      <label for="openai-api-key">OpenAI API Key (optional):</label>
      <input type="password" id="openai-api-key" placeholder="Enter your OpenAI API key">
      <br>
      <label for="openai-endpoint">Endpoint URL (optional):</label>
      <input type="text" id="openai-endpoint" placeholder="https://api.openai.com (default)">

    </div>

  </div>
  
      <div style="margin: 30px 0;">
      <h3>Default Image</h3>
      <img height="300px" id="htmlhandwritten" src="knocknoc2.jpg" style="border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
      <p>knock knock credit <a href="https://www.johncarr.online/knock-knock-jokes/">johncarr.online/knock-knock-jokes</a></p>
    </div>


  <p><a href="https://github.com/naptha/tesseract.js">tesseract.js</a> to perform OCR on the camera frame.</p>

  <!--  <img src="https://herebox.org/wp-content/uploads/2025/07/the-triplets-of-belleville-749x1024.jpg"> -->
 
    <script type="module">
        import tesseractJs from "https://esm.sh/tesseract.js@6.0.1";

        // Get the image and log elements from the DOM
        const imageElement = document.getElementById('htmlhandwritten');
        const logElement = document.getElementById('log');
        
        // Initialize CodeMirror editor
        let codeMirrorEditor;
        document.addEventListener('DOMContentLoaded', function() {
          codeMirrorEditor = CodeMirror(document.getElementById('codemirror-container'), {
            mode: 'htmlmixed',
            theme: 'solarized dark',
            lineNumbers: true,
            autoCloseBrackets: true,
            matchBrackets: true,
            indentUnit: 2,
            tabSize: 2,
            indentWithTabs: false,
            lineWrapping: true,
            placeholder: 'OCR results will appear here...',
            value: '',
            scrollbarStyle: 'null',
            lineWiseCopyCut: false
          });
          
          // Set default height to 15 lines (approximately 15 * 1.4 * 14px = 294px)
          codeMirrorEditor.setSize(null, '294px');
          
          // Make the editor resizable
          codeMirrorEditor.refresh();
        });
        
        // OCR engine selection elements
        const ocrEngineRadios = document.querySelectorAll('input[name="ocr-engine"]');
        const openaiConfig = document.getElementById('openai-config');
        const openaiApiKey = document.getElementById('openai-api-key');
        const openaiEndpoint = document.getElementById('openai-endpoint');
        
        // Function to get selected OCR engine
        function getSelectedOCREngine() {
          const selectedRadio = document.querySelector('input[name="ocr-engine"]:checked');
          if (!selectedRadio) {
            // Default to tesseract if no radio button is selected
            const tesseractRadio = document.querySelector('input[value="tesseract"]');
            if (tesseractRadio) {
              tesseractRadio.checked = true;
              return 'tesseract';
            }
            return 'tesseract'; // fallback
          }
          return selectedRadio.value;
        }
        
        // Function to update UI based on OCR engine selection
        function updateOCREngineUI() {
          const selectedEngine = getSelectedOCREngine();
          
          // Hide all config sections
          openaiConfig.style.display = 'none';
          
          // Show the appropriate config section
          if (selectedEngine === 'openai') {
            openaiConfig.style.display = 'block';
          }
        }
        
        // Event listeners for OCR engine selection
        ocrEngineRadios.forEach(radio => {
          radio.addEventListener('change', updateOCREngineUI);
        });
        
        // Function to update the log/status on the page
        function updateLog(status) {
          logElement.innerHTML = `<p>${status}</p>`;
          console.log(status);
        }
        
        // Function to convert canvas to base64
        function canvasToBase64(canvas) {
          return canvas.toDataURL('image/jpeg', 0.8);
        }
        

        
        // OpenAI GPT-4 Vision OCR function
        async function performOpenAIOCR(imageData) {
          const apiKey = openaiApiKey.value.trim();
          
          updateLog('Performing OCR with OpenAI GPT-4 Vision...');
          
          const prompt = `Please perform OCR (Optical Character Recognition) on this image and extract all the text content. Return only the extracted text without any additional formatting or explanations.`;
          
          const requestBody = {
            model: "gpt-4o",
            messages: [
              {
                role: "user",
                content: [
                  {
                    type: "text",
                    text: prompt
                  },
                  {
                    type: "image_url",
                    image_url: {
                      url: imageData
                    }
                  }
                ]
              }
            ],
            max_tokens: 1000,
            temperature: 0.1
          };
          
          try {
            // Use Digital Ocean Functions proxy
            const response = await fetch('https://faas-sfo3-7872a1dd.doserverless.co/api/v1/web/fn-00585b33-7c19-4732-b11d-1ca044549063/default/ocr-proxy', {
              method: 'POST',
              headers: {
                'Content-Type': 'application/json'
              },
              body: JSON.stringify({
                method: 'POST',
                path: 'chat/completions',
                headers: {
                  'authorization': apiKey ? `Bearer ${apiKey}` : null
                },
                body: JSON.stringify(requestBody),
                api_type: 'openai'
              })
            });
            
            if (!response.ok) {
              const errorText = await response.text();
              throw new Error(`OpenAI API error: ${response.status} - ${errorText}`);
            }
            
            const result = await response.json();
            console.log('Proxy response:', result);
            
            // Check if the response is a direct OpenAI response or wrapped in a proxy response
            if (result.choices && result.choices[0] && result.choices[0].message) {
              // Direct OpenAI response
              console.log('Direct OpenAI response detected');
              return result.choices[0].message.content.trim();
            } else if (result.statusCode === 200 && result.body) {
              // Wrapped proxy response
              try {
                let responseBody;
                if (typeof result.body === 'string') {
                  responseBody = JSON.parse(result.body);
                } else {
                  responseBody = result.body;
                }
                console.log('Parsed response body:', responseBody);
                if (responseBody.choices && responseBody.choices[0] && responseBody.choices[0].message) {
                  return responseBody.choices[0].message.content.trim();
                } else {
                  throw new Error('Unexpected response format from OpenAI API');
                }
              } catch (parseError) {
                console.error('Failed to parse response body:', result.body);
                console.error('Parse error:', parseError);
                throw new Error(`Failed to parse OpenAI response: ${parseError.message}`);
              }
            } else if (result.statusCode && result.statusCode !== 200) {
              // Error response
              try {
                const errorBody = typeof result.body === 'string' ? JSON.parse(result.body) : result.body;
                throw new Error(`OpenAI API error: ${result.statusCode} - ${errorBody.error || errorBody}`);
              } catch (parseError) {
                console.error('Failed to parse error body:', result.body);
                throw new Error(`OpenAI API error: ${result.statusCode} - ${result.body}`);
              }
            } else {
              throw new Error('Unexpected response format from proxy');
            }
            
          } catch (error) {
            console.error('OpenAI API error:', error);
            throw error;
          }
        }
        

        
        // Camera functionality
        const videoElement = document.getElementById('camera-video');
        const startButton = document.getElementById('start-camera');
        const stopButton = document.getElementById('stop-camera');
        const ocrButton = document.getElementById('ocr-camera');
        let stream = null;
        
        // Start camera function
        async function startCamera() {
          try {
            updateLog('Requesting camera permissions...');
            
            // Request camera access with specific constraints for mobile
            stream = await navigator.mediaDevices.getUserMedia({
              video: {
                facingMode: 'environment', // Use back camera on mobile
                width: { ideal: 1280 },
                height: { ideal: 720 }
              },
              audio: false
            });
            
            // Set the video source
            videoElement.srcObject = stream;
            
            updateLog('Camera started successfully!');
            startButton.style.display = 'none';
            stopButton.style.display = 'inline-block';
            
          } catch (error) {
            console.error('Camera access failed:', error);
            updateLog(`Camera access failed: ${error.message}`);
          }
        }
        
        // Stop camera function
        function stopCamera() {
          if (stream) {
            stream.getTracks().forEach(track => track.stop());
            stream = null;
            videoElement.srcObject = null;
            updateLog('Camera stopped');
            startButton.style.display = 'inline-block';
            stopButton.style.display = 'none';
          }
        }
        
        // OCR function for camera video frame
        async function performOCROnCamera() {
          if (!stream || !videoElement.srcObject) {
            updateLog('Please start the camera first');
            return;
          }
          
          const selectedEngine = getSelectedOCREngine();
          
          // Check credentials for AI engines
          if (selectedEngine === 'openai') {
            const apiKey = openaiApiKey.value.trim();
            if (!apiKey) {
              updateLog('OpenAI selected but API key not configured. Please enter your OpenAI API key, or switch to Tesseract.');
              return;
            }
          }
          
          updateLog(`Performing OCR on camera frame using ${selectedEngine}...`);
          
          try {
            // Get the display canvas for showing the captured frame
            const displayCanvas = document.getElementById('captured-frame');
            const displayCtx = displayCanvas.getContext('2d');
            
            // Set display canvas size to match video dimensions
            displayCanvas.width = videoElement.videoWidth;
            displayCanvas.height = videoElement.videoHeight;
            
            // Draw the current video frame to the display canvas
            displayCtx.drawImage(videoElement, 0, 0, displayCanvas.width, displayCanvas.height);
            
            // Create a temporary canvas for OCR processing
            const canvas = document.createElement('canvas');
            const ctx = canvas.getContext('2d');
            
            // Set canvas size to match video dimensions
            canvas.width = videoElement.videoWidth;
            canvas.height = videoElement.videoHeight;
            
            // Draw the current video frame to the canvas
            ctx.drawImage(videoElement, 0, 0, canvas.width, canvas.height);
            
            // Store the captured frame for retry functionality
            lastCapturedFrame = canvasToBase64(canvas);
            
            // Update the captured frame display
            const cameraDisplayCanvas = document.getElementById('captured-frame');
            const cameraDisplayCtx = cameraDisplayCanvas.getContext('2d');
            cameraDisplayCanvas.width = canvas.width;
            cameraDisplayCanvas.height = canvas.height;
            cameraDisplayCtx.drawImage(canvas, 0, 0);
            
            updateLog('ðŸ“¸ New snapshot captured from camera');
            updateLog('ðŸ” Click "Try OCR text recognition" to extract text');
            
            if (selectedEngine === 'openai') {
              // Use OpenAI for camera OCR
              const imageData = canvasToBase64(canvas);
              const text = await performOpenAIOCR(imageData);
              
                              // Store the extracted text in CodeMirror
                if (codeMirrorEditor) {
                  codeMirrorEditor.setValue(text);
                }
                
                // Show the open text tab button
                document.getElementById('open-text-tab').style.display = 'inline-block';
                
                // Display the results
                updateLog('Camera OpenAI OCR Complete!');
              
                        } else {
              // Use Tesseract for camera OCR
              const result = await tesseractJs.recognize(
                canvas,
                'eng',
                {
                  logger: m => {
                    const progressStatus = `Camera OCR: ${m.status} (${(m.progress * 100).toFixed(2)}%)`;
                    updateLog(progressStatus);
                  }
                }
              );
              
              // Debug: Log the full result structure
              console.log('Camera OCR result:', result);
              
              const { data: { text, words } } = result;
              
              // Store the extracted text in CodeMirror
              if (codeMirrorEditor) {
                codeMirrorEditor.setValue(text);
              }
              
              // Show the open text tab button
              document.getElementById('open-text-tab').style.display = 'inline-block';
              
              // Display the results
              updateLog('Camera Tesseract OCR Complete!');
              
              // Display individual words and their confidence levels (if available)
              if (words && Array.isArray(words)) {
                updateLog(`Individual words detected: ${words.length} words`);
                words.forEach(word => {
                  updateLog(`"${word.text}" (confidence: ${(word.confidence * 100).toFixed(2)}%)`);
                });
              } else {
                updateLog('Word-level data not available');
              }
            }
            
          } catch (error) {
            console.error('Camera OCR failed:', error);
            updateLog(`Camera OCR failed: ${error.message}`);
          }
        }
        
        // Global variable to store the last captured frame
        let lastCapturedFrame = null;
        
        // Retry OCR function using the last captured frame
        async function retryOCR() {
          console.log('Retry OCR called');
          
          // If no lastCapturedFrame, use the default image
          let imageToProcess;
          let isDefaultImage = false;
          
          if (!lastCapturedFrame) {
            // Use the default image if no camera snapshot has been taken
            imageToProcess = imageElement;
            isDefaultImage = true;
            updateLog('Using default image (knock knock joke) for OCR...');
            console.log('Using default image:', imageElement);
          } else {
            // Use the last captured frame
            imageToProcess = lastCapturedFrame;
            updateLog('Using last captured frame for OCR...');
            console.log('Using last captured frame');
          }
          
          const selectedEngine = getSelectedOCREngine();
          
          // Check credentials for AI engines
          if (selectedEngine === 'openai') {
            const apiKey = openaiApiKey.value.trim();
            if (!apiKey) {
              updateLog('OpenAI selected but no API key provided in UI. Trying to use environment variable from Digital Ocean Function...');
              // Continue without API key - the function will try to use environment variable
            }
          }
          
          updateLog(`Performing OCR using ${selectedEngine}...`);
          console.log('Selected engine:', selectedEngine);
          
          try {
            if (selectedEngine === 'openai') {
              // For OpenAI, we need to convert the image to base64
              let imageData;
              if (isDefaultImage) {
                const canvas = document.createElement('canvas');
                const ctx = canvas.getContext('2d');
                canvas.width = imageToProcess.naturalWidth;
                canvas.height = imageToProcess.naturalHeight;
                ctx.drawImage(imageToProcess, 0, 0);
                imageData = canvasToBase64(canvas);
              } else {
                imageData = imageToProcess;
              }
              
              const text = await performOpenAIOCR(imageData);
              
              // Store the extracted text in CodeMirror
              if (codeMirrorEditor) {
                codeMirrorEditor.setValue(text);
              }
              
              // Show the open text tab button
              document.getElementById('open-text-tab').style.display = 'inline-block';
              
              // Display the results
              updateLog('OpenAI OCR Complete!');
              
            } else if (selectedEngine === 'tesseract') {
              // Use Tesseract for OCR
              console.log('Using Tesseract for OCR with image:', imageToProcess);
              updateLog('Note: Tesseract.js may download language models on first use (this is normal)');
              const result = await tesseractJs.recognize(
                imageToProcess,
                'eng',
                {
                  logger: m => {
                    const progressStatus = `OCR: ${m.status} (${(m.progress * 100).toFixed(2)}%)`;
                    updateLog(progressStatus);
                  }
                }
              );
              
              const { data: { text, words } } = result;
              
              // Store the extracted text in CodeMirror
              if (codeMirrorEditor) {
                codeMirrorEditor.setValue(text);
              }
              
              // Show the open text tab button
              document.getElementById('open-text-tab').style.display = 'inline-block';
              
              // Display the results
              updateLog('Tesseract OCR Complete!');
              
              // Display individual words and their confidence levels (if available)
              if (words && Array.isArray(words)) {
                updateLog(`Individual words detected: ${words.length} words`);
                words.forEach(word => {
                  updateLog(`"${word.text}" (confidence: ${(word.confidence * 100).toFixed(2)}%)`);
                });
              }
                          } else {
                updateLog(`Unknown OCR engine: ${selectedEngine}. Please select Tesseract or OpenAI.`);
              }
          } catch (error) {
            updateLog(`OCR failed: ${error.message}`);
            console.error('OCR error details:', error);
            if (error.message.includes('NetworkError')) {
              updateLog('This may be due to network issues or firewall blocking Tesseract model downloads.');
              updateLog('Try refreshing the page or check your internet connection.');
            }
          }
        }
        
        // Event listeners for camera buttons
        startButton.addEventListener('click', startCamera);
        stopButton.addEventListener('click', stopCamera);
        ocrButton.addEventListener('click', performOCROnCamera);
        
        // Function to open extracted text in new tab
        function openTextInNewTab() {
          if (!codeMirrorEditor) {
            updateLog('CodeMirror editor not initialized yet. Please wait a moment and try again.');
            return;
          }
          
          const text = codeMirrorEditor.getValue().trim();
          
          if (!text) {
            updateLog('No text available. Please perform OCR first or enter text manually.');
            return;
          }
          
          const newWindow = window.open('', '_blank');
          newWindow.document.write(text);
          newWindow.document.close();
          
          updateLog('âœ… Text opened in new tab');
        }
        

        
        // Add event listener for retry button
        const retryButton = document.getElementById('retry-ocr');
        retryButton.addEventListener('click', retryOCR);
        
        // Add event listener for open text tab button
        const openTextTabButton = document.getElementById('open-text-tab');
        openTextTabButton.addEventListener('click', openTextInNewTab);
        

        

        
        // Initially hide the stop button
        stopButton.style.display = 'none';
        
        // Initialize with default image OCR
        async function initializeWithDefaultImage() {
          updateLog('Setting up default image (knock knock joke) for OCR...');
          
          // Ensure Tesseract is selected by default
          document.querySelector('input[value="tesseract"]').checked = true;
          updateOCREngineUI();
          
          try {
            // Check if image is properly loaded
            if (!imageElement.complete || imageElement.naturalWidth === 0) {
              updateLog('Waiting for image to load completely...');
              await new Promise((resolve) => {
                imageElement.onload = resolve;
                imageElement.onerror = () => {
                  throw new Error('Failed to load default image');
                };
              });
            }
            
            // Create a canvas to draw the default image
            const canvas = document.createElement('canvas');
            const ctx = canvas.getContext('2d');
            
            // Set canvas size to match the image
            canvas.width = imageElement.naturalWidth;
            canvas.height = imageElement.naturalHeight;
            
            // Draw the default image to the canvas
            ctx.drawImage(imageElement, 0, 0);
            
            // Store as the last captured frame
            lastCapturedFrame = canvasToBase64(canvas);
            
            // Update the captured frame display to show the default image
            const displayCanvas = document.getElementById('captured-frame');
            const displayCtx = displayCanvas.getContext('2d');
            displayCanvas.width = canvas.width;
            displayCanvas.height = canvas.height;
            displayCtx.drawImage(canvas, 0, 0);
            
            updateLog('âœ… Default image loaded and ready for OCR');
            updateLog('ðŸ“¸ Current snapshot: Knock knock joke image');
            updateLog('ðŸ” Click "Try OCR text recognition" to extract text');
            
            // Try to perform initial OCR, but don't fail if there are security issues
            try {
              updateLog('Performing OCR on default image using Tesseract.js...');
              
              // Use Tesseract for default image OCR
              updateLog('Note: Tesseract.js may download language models on first use (this is normal)');
              const result = await tesseractJs.recognize(
                canvas,
                'eng',
                {
                  logger: m => {
                    const progressStatus = `Default Image OCR: ${m.status} (${(m.progress * 100).toFixed(2)}%)`;
                    updateLog(progressStatus);
                  }
                }
              );
              
              const { data: { text, words } } = result;
              
              // Store the extracted text in CodeMirror
              if (codeMirrorEditor) {
                codeMirrorEditor.setValue(text);
              }
              
              // Show the open text tab button
              document.getElementById('open-text-tab').style.display = 'inline-block';
              
              updateLog('Default image Tesseract OCR Complete!');
              
              // Display individual words and their confidence levels (if available)
              if (words && Array.isArray(words)) {
                updateLog(`Individual words detected: ${words.length} words`);
                words.forEach(word => {
                  updateLog(`"${word.text}" (confidence: ${(word.confidence * 100).toFixed(2)}%)`);
                });
              }
            } catch (ocrError) {
              updateLog(`Initial OCR failed: ${ocrError.message}`);
              updateLog('You can still use the "Try OCR text recognition" button to perform OCR manually.');
            }
          } catch (error) {
            updateLog(`Default image OCR failed: ${error.message}`);
            if (error.message.includes('NetworkError')) {
              updateLog('This may be due to network issues or firewall blocking Tesseract model downloads.');
              updateLog('Try refreshing the page or check your internet connection.');
            } else if (error.message.includes('insecure')) {
              updateLog('This error is likely due to CORS restrictions when loading from a file:// URL.');
              updateLog('Please serve the page from a web server (e.g., python3 -m http.server) instead of opening the file directly.');
              updateLog('The OCR will work once the page is served from a proper web server.');
            }
          }
        }
        
        // Wait for the image to load, then initialize
        if (imageElement.complete) {
          initializeWithDefaultImage();
        } else {
          imageElement.addEventListener('load', initializeWithDefaultImage);
        }
        
        // Initialize OCR engine UI after DOM is ready
        updateOCREngineUI();
    </script>

</html>